\chapter{Descriptive Complexity}\label{ch:descriptive-complexity}


\section{Aims}\label{sec:aims}

In mathematics, abstraction is one of the most important tools as it enables us to make general statements and prove them for all the concrete instantiations of a concept.
Formal Logic takes this even further and makes it possible to abstract mathematical thought itself.
In Computer science, we are often interested in the amount of resources needed to compute a certain function or solve a certain problem, speaking in terms of time and storage space.
Different forms of logic have the power to describe different types of problems.
By focusing on decision problems\footnote{Any problem can be reduced to boolean queries, for example by having a boolean querry meaning "the i$^{th}$ bit of an encoding of the answer is 1"}, we can say a corresponding logical charaterisation of a problem is a formula $\varphi$ which is true if and only if a structure satisfies the required properties.
By looking at the complexity of formulas which are needed to describe problems in terms of relations, operators, variables and other metrics, we can often find remarkably natural classes of logic corresponding to classes of problems.

Using these results, many insights into the underlying structure of real-world problems can be made which in turn can give us better ways to deal with them.
Further, descriptive complexity has applications in database theory and computer aided verification and proofs.


\section{Tools}\label{sec:tools}

As always, we first need to present some tools and techniques which will be used later in the proofs.
Definition are again taken and modified from~\cite{theory-cs} and~\cite{descriptive-complexity}.

\subsection{Complexity Theory}\label{subsec:complexity-theory}

Complexity theory is the study of the resources, measured mostly in time and space, needed to compute certain problems\footnote{The specific model of computation is not important as all give almost the same results. We will assume turing machines}.
Also, we do not really care about constants in the computation, and thus use a notation which omits these.

\begin{define}[Big-O notation]
    Let $f, g$ be functions $f, g: \mathbb{N} \to \mathbb{R}_+$.

    We say that $f \in \mathcal{O}(g)$ if there exists positive integers $n_0, c$ such that for all $n \geq n_0$ we have \[f(n) \leq c\cdot g(n)\]
\end{define}

Complexity classes can then be defined as all the problems which have a turing machine satisfying some bounds that can compute their solutions.
Now we will define some common and important complexity classes.

\begin{define}
    [{DTIME[$\mathcal{O}(t)$]}]
    We say that a decision problem is in DTIME[$\mathcal{O}(t)$] if there exists a deterministic turing machine that takes a maximum of $f(n)$ steps on any input of size $n$ and $f \in \mathcal{O}(t)$.
\end{define}

\begin{define}[P]
    We say that a decision problem is in P if there exists a polynomial $q$ such that the problem is in DTIME[$\mathcal{O}(q)$]
\end{define}

\begin{define}
    [{DSPACE[$\mathcal{O}(t)$]}]
    We say that a decision problem is in DTIME[$\mathcal{O}(t)$] if there exists a deterministic turing machine that visits a maximum of $f(n)$ tape cells on any input of size $n$ and $f \in \mathcal{O}(t)$.
\end{define}

\begin{define}[PSPACE]
    We say that a decision problem is in PSPACE if there exists a polynomial $q$ such that the problem is in DSPACE[$\mathcal{O}(q)$]
\end{define}

We can go do the same for nondeterministic turing machines, and get the corresponding complexity classes NTIME, NP, NSPACE, NPSPACE.
There, we always take the maximum of tape cells and steps over any computation branch.

The complexity class P has a special meaning for computer scientists as these are the problems which are deemed "feasible" on modern computers.

\subsection{Reduction and Completeness}\label{subsec:reduction}

A reduction can informally be seen as a method of using a problem we already solved to solve a new problem by converting this new problem into an instance of the old problem.
These reduction can be very usefull to define complete problems for complexity classes, which in turn enable us to prove theorems for all problems of a specific complexity class.

\begin{define}[first-order reduction]
    Let $\mathcal{C}$ be a complexity class and $A$ and $B$ be two problems over vocabularies $\sigma$ and $\tau$.
    Now suppose that there is some first-order query $I: \text{STRUC}[\sigma] \to \text{STRUC}[\tau]$ for which we have the following property:
    \[
        \mathcal{A} \in A \Leftrightarrow I(\mathcal{A}) \in B
    \]
    Then $I$ is a first order reduction from $A$ to $B$, denoted as $A \leq_{fo} B$.
\end{define}

First order reductions can then be used to show that some problem is also a member in some complexity class, as in most complexity classes, we can compute the first order query, and then we are left with a problem that we already know is in the required class.
The converse can also be shown: for some problem $B$ which is not in some complexity class $\mathcal{C}$, if we have $B \leq_{fo} A$, then $A$ is also not in $\mathcal{C}$, as otherwise $B$ would also be in $\mathcal{C}$, which is a contradiction.

Using the reductions, we can define completeness.

\begin{define}[Completness via first-order reductions for Complexity Class $\mathcal{C}$]
    We say some problem $A$ is complete for $\mathcal{C}$ via $\leq_{fo}$ if and only if
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item $A \in \mathcal{C}$
        \item for all $B \in \mathcal{C}$, we have $B \leq_{fo} A$
    \end{itemize}
\end{define}

Informally, a complete problem captures the essence of the complexity class.
Further, they have an application in some proofs of equivalences between complexity classes $\mathcal{C}$ and logics $\mathcal{L}$.
These proofs follow the following steps as in~\cite{descriptive-complexity}:
\begin{enumerate}
    \item Show that $\mathcal{L} \subseteq \mathcal{C}$ by providing a way to convert any formula $\varphi \in \mathcal{L}$ into an algorithm in $\mathcal{C}$.
    \item Find a complete problem $T$ for $\mathcal{C}$ via first-order reductions.
    \item Show that $\mathcal{L}$ is closed under first-order reductions, that is that any formula can be extended by first-order quantifiers and boolean connectives and stay in $\mathcal{L}$.
    \item Find a formula for $T$ in $\mathcal{L}$, which shows $T \in \mathcal{L}$.
\end{enumerate}
The above steps work, as for any problem $B$ in $\mathcal{C}$, there is a first-order reduction $I$ to $T$, and both $\mathcal{L}$ and $\mathcal{C}$ are complete via these reductions, so we also have $B \in \mathcal{L} = \mathcal{C}$.

\subsection{Ehrenfeucht-Fraïssé Games}\label{subsec:ehrenfeucht-fraisse-games}

Ehrenfeucht-Fraïssé games are combinatorial games which are equivalent to first-order formulas and their extensions.
Using these games, it is often possible to show non-expressability results for certain problems in some logic $\mathcal{L}$.

As a motivation, we can look at what it means for a formula to hold on some structure.
Assume the formula has the form $\forall x\varphi(x)$.
Then this can be seen as some opponent choosing some element $a \in |\mathcal{A}|$ and us now needing to show that $\varphi(a)$ holds.
The case where the formula has the form $\exists x\psi(x)$ can be treated similarly, but we can choose the element ourselves.

Now for the formal definition
\begin{define}[Ehrenfeucht-Fraïssé Game]
    The \emph{$k$-pebble Ehrenfeucht-Fraïssé Game $\mathcal{G}_k$} is played by two players: the Spoiler and the Duplicator on a pair of structures $\mathcal{A}$ and $\mathcal{B}$ using $k$ pairs of pebbles.
    In each move, the spoiler places one of the remaining pebbles on an element of one of the two structures.
    Then, the duplicator tries to match the move on the other structure by placing the corresponding pebble on an element.
    We say that the duplicator wins the $k$-pebble Ehrenfeucht-Fraïssé Game on $\mathcal{A}, \mathcal{B}$ if after the $k$ rounds, the map $i : |\mathcal{A}| \to |\mathcal{B}|$ defined as for all elements of $|\mathcal{A}|$ with a pebble and the constants as the element in $|\mathcal{B}|$ with the corresponding pebble or constant forms a partial isomorphism.
    A partial isomorphism is an isomorphism formed for some subset of the universe, with all relations restricted to that subset.
\end{define}

In this context, the spoiler wants to show that $\mathcal{A}$ and $\mathcal{B}$ are different, whereas the duplicator wants to show their equivalence.

As this is a zero-sum game of full information, one of the two players must have a winning strategy.
It can be proven that if the duplicator has a winning strategy for the $k$-pebble Ehrenfeucht-Fraïssé on $\mathcal{A}$ and $\mathcal{B}$ if and only if $\mathcal{A}$ and $\mathcal{B}$ agree on all formulas with less or equal to $k$ nested quantifiers.
We can use these facts to prove non-expressibility of some problems in first-order logic by exhibiting two structures $\mathcal{A}_k$ and $\mathcal{B}_k$ for each $k$ with one satisfying the problem constraints and the other not and a winning strategy for the duplicator on these two structures.
This methodology can be extended to other logics by adding new moves or restrictions to the game.

\section{Important Results}\label{sec:important-results}

\subsection{NSPACE[$s(n)$] $\subseteq$ DSPACE[$s(n)^2$]}\label{subsec:nspacesubsetdspacesquared}

\subsection{SPACE Hierarchy theorem}\label{subsec:space-hierarchy-theorem}


\section{Results concerning the Chomsky hierarchy}\label{sec:results-concerning-the-chomsky-hierarchy}

\subsection{Regular Languages}\label{subsec:des-regular-languages}

\subsection{Context-Free Languages}\label{subsec:des-context-free-languages}

\subsection{Context-Sensitive Languages}\label{subsec:des-context-sensitive-languages}

\subsection{Recursive Languages}\label{subsec:des-recursive-languages}


\section{Open questions}\label{sec:open-questions}

\subsection{P$\overset{?}{=}$NP}\label{subsec:pnp}

\subsection{NSPACE[$O(n)$]$\overset{?}{=}$DSPACE[$O(n)$]}\label{subsec:nspacedspace}
%! suppress = UnresolvedReference
%! suppress = MissingImport


\chapter{Descriptive Complexity}\label{ch:descriptive-complexity}


\section{Aims}\label{sec:aims}

In mathematics, abstraction is one of the most important tools, as it enables us to make general statements and prove them for all concrete instances of a concept.
Formal logic takes this even further and makes it possible to abstract mathematical thought itself.
That is the reason why {Descriptive} Complexity uses formal logic to describe computational problems in a way that captures their mathematical essence.

In computer science, we are always interested in the amount of resources needed to compute some function or solve a certain problem, speaking in terms of time and storage space.
By focusing on decision problems\footnote{Any problem can be reduced to boolean queries, for example by having a boolean query meaning ``the i$^{th}$ bit of an encoding of the answer is 1''}, we can define a logical characterization of a problem as a formula $\varphi$ which is true if and only if a structure satisfies the required properties.
By looking at the complexity of formulas that are needed to describe problems in terms of relations, operators, variables, and other metrics, we can often find remarkably natural classes of logic that correspond to computational classes of problems.

Using results from this research, many insights into the underlying structure of real-world problems can be made, which in turn can lead to better ways to tackle them.
Further, Descriptive Complexity has applications in database theory, computer-aided verification, and proofs.


\section{Tools}\label{sec:tools}

Some tools and techniques which are used later for proofs need to be introduced first.
In this section, only Complexity theory is presented.
In \cref{subsec:ehrenfeucht-fraisse-games} and \cref{subsec:reduction},   Ehrenfeucht-Fraïssé games and first-order reductions are also presented.
These tools are not needed for \cref{ch:personal-contribution}, but I tried using them extensively during the research phase of this project.
Definitions are again taken from~\cite{theory-cs} and~\cite{descriptive-complexity}.

\subsection{Complexity Theory}\label{subsec:complexity-theory}

Complexity theory is the study of the resources, measured mostly in time and space, needed to compute the answers to computational problems\footnote{The specific model of computation is not important as all give almost the same results. We assume Turing machines, which are defined in \cref{sec:turing-machines}.}.
As different computers can handle tasks at various speeds, a notation which only considers the asymptotic use of resources is used.

\begin{define}[Big-O notation]
    Let $f, g$ be functions $f, g: \mathbb{N} \to \mathbb{R}_+$.

    We say that $f(n) \in \mathcal{O}(g(n))$ if there exists positive integers $n_0, c$ such that for all $n \geq n_0$ we have \[f(n) \leq c\cdot g(n)\]
\end{define}

Complexity classes can be defined as the set of all problems for which there exists a Turing machine satisfying some bounds that computes the answer to the problem.
Now, some common complexity classes are defined.

\begin{define}
[{\acs{DTIME}$[t(n)]$}]
    We say that a decision problem is in \acs{DTIME}$[t(n)]$ if there exists a deterministic Turing machine that takes a maximum of $f(n)$ steps on any input of size $n$ and $f(n) \in \mathcal{O}(t(n))$.
\end{define}

\begin{define}[\acs{P}]
    We say that a decision problem is in \acs{P} if there exists a polynomial $q$ such that the problem is in \acs{DTIME}$[q(n)]$.
\end{define}

\begin{define}
[{\acs{DSPACE}$[t(n)]$}]
    We say that a decision problem is in \acs{DSPACE}$[t(n)]$ if there exists a deterministic Turing machine that visits a maximum of $f(n)$ tape cells on any input of size $n$ and $f(n) \in \mathcal{O}(t(n))$.
\end{define}

\begin{define}[\acs{PSPACE}]
    We say that a decision problem is in \acs{PSPACE} if there exists a polynomial $q$ such that the problem is in \acs{DSPACE}$[q(n)]$.
\end{define}

We can do the same for nondeterministic Turing machines, and get the corresponding complexity classes \acs{NTIME}, \acs{NP}, \acs{NSPACE}, \acs{NPSPACE}\@.
There, we always take the maximum of tape cells and steps over any computation branch.

The complexity class \acs{P} has a special meaning for computer scientists, as the problems in \acs{P} are deemed ``feasible'' on modern computers.


\section{Important Results}\label{sec:important-results}

In this section, only Savitch's theorem is introduced, which Savitch presented in \cite{Savitch1970}.
In \cref{subsec:space-hierarchy-theorem}, the space hierarchy theorem is presented.

\subsection{Savitch's Theorem}\label{subsec:nspacesubsetdspacesquared}

Savitch's theorem relates nondeterministic space-bounded Turing machines with deterministic space-bounded Turing machines.
As an intermediate step, it introduces alternating Turing machines.
These Turing machines are a generalization of nondeterministic Turing machines, which can be seen as machines taking the ``or'' over all their computation paths.

\begin{define}[Alternating Turing Machine]
    An alternating Turing machine is a Turing machine with two types of states: existential and universal.
    The accepting conditions change compared to a nondeterministic Turing machine and depend on the current state of the alternating Turing machine.
    If it is in an existential state, the current configuration is accepting if and only if \emph{at least one} of the directly reachable configurations is accepting.
    If the machine is in a universal state, the current configuration is accepting if and only if \emph{all} the directly reachable configurations are accepting.
\end{define}

In addition to taking the ``or'' over its children, an alternating Turing machine can thus also take the ``and''.

\sloppy We define \acs{ATIME}$[t(n)]$ and \acs{ASPACE}$[t(n)]$ analogously to \acs{NTIME}$[t(n)]$ and \acs{NSPACE}$[t(n)]$ in \cref{subsec:complexity-theory}.

We can proceed to the proof of Savitch's theorem using the technique presented in~\cite{descriptive-complexity}.
\begin{theorem}[Savitch's Theorem]
    For all space constructible functions with $t(n) \geq \log n$ we have
    \[
        \text{\upshape \acs{NSPACE}}[t(n)] \subseteq \text{\upshape \acs{ATIME}}[t(n)^2] \subseteq \text{\upshape \acs{DSPACE}}[t(n)^2]
    \]
\end{theorem}

\begin{proof}
    We start with the first inclusion, $\text{\acs{NSPACE}$[t(n)]$} \subseteq \text{\acs{ATIME}$[t(n)^2]$}$.
    We need to show that any \acs{NSPACE}$[t(n)]$ Turing machine can be simulated by an \acs{ATIME}$[t(n)^2]$ alternating Turing machine.
    Let $N$ be a \acs{NSPACE}$[t(n)]$ Turing machine.
    Without loss of generality, we assume that $N$ clears its tape after accepting and goes back to the first cell.

    Consider $G_w$, the computation graph of $N$ on input $w$.
    This graph consists of configurations as vertices and directed edges from each configuration to all configurations that are directly reachable from it.
    We see that $N$ accepts $w$ if and only if there is a path from the start configuration $s$ to the accepting configuration $t$ in $G_w$.
    We now present a routine $P(d, x, y)$ which asserts that there is a path of length at most $2^{d}$ from vertex $x$ to $y$.
    Recursively, we can define $P$ as follows:
    \[
        P(d, x, y) = (\exists z)(P(d - 1, x, z) \land P(d - 1, z, y))
    \]
    This formula asserts that there exists a middle vertex $z$ for which there is a path of length at most $2^{d - 1}$ from $x$ to $z$ and from $z$ to $y$.
    The base case for $d = 0$ is to check whether $x$ can be reached from $y$ by a transition in the transition table of $N$.
    Using an alternating Turing machine, the formula can be evaluated by using existential states to find the middle vertex $z$, and then a universal state choosing which one of the two new paths should be checked.

    For the runtime analysis, we proceed as follows.
    It takes $\mathcal{O}(t(n))$ time to write down the middle vertex $z$, as each configuration includes the tape contents, which have length $\mathcal{O}(t(n))$.
    Further, one of the new shorter paths $P(d - 1, z, y)$ is then evaluated.
    By induction, we find that we need $\mathcal{O}(d\cdot t(n))$ time to compute $P(d, x, y)$.
    There are only $2^{\mathcal{O}(t(n))}$ possible configurations, so we get that the initial $d$ is also in $\mathcal{O}(t(n))$, and thus our total runtime is $\mathcal{O}(t(n)\cdot t(n)) = \mathcal{O}(t(n)^2)$.
    
    \vspace{5mm}
    
    \sloppy For the second inclusion, by substituting $s(n)$ for $t(n)^2$, we need to simulate an \acs{ATIME}$[s(n)]$ Turing machine $A$ using a \acs{DSPACE}$[s(n)]$ Turing machine.
    Again, we consider the computation graph of $A$ of on input $w$.
    This graph has depth $\mathcal{O}(s(n))$ and size $2^{\mathcal{O}(s(n))}$.

    A \acs{DSPACE}$[s(n)]$ Turing machine $D$ can systematically search this computation graph to simulate $A$.
    This is done by keeping a string of choices $c_{1}c_{2}\dots c_r$ of length $\mathcal{O}(s(n))$ made up to this point.
    Each choice takes up $\mathcal{O}(1)$ place, as the number of transitions from one state is bounded by a constant depending only on $A$.
    Note that these choices uniquely determine which state we are in.

    Now, $D$ can find the answer recursively.
    If in the simulation, $A$ is in a halt state, $D$ reports this back to the previous state.
    In an existential state, $D$ simulates the children of that state, and if it gets a positive result from one of them, $D$ also reports a positive result.
    In a universal state, $D$ simulates the children of that state, and if it gets a positive result from all of them, $D$ reports a positive result.

    Overall, $D$ only uses $\mathcal{O}(s(n))$ space to simulate $A$.
    This shows that we can simulate an \acs{ATIME}$[t(n)^2]$ Turing machine $A$ using a \acs{DSPACE}$[t(n)^2]$ Turing machine.

    Thus, the second part of the theorem follows and by the transitivity of $\subseteq$ we have $\text{\acs{NSPACE}}[t(n)] \subseteq \text{\acs{DSPACE}}[t(n)^2]$.
\end{proof}

It is not known if the containment is strict for any of the inclusions.
Neither is it known if the quadratic overhead for simulating nondeterministic space is optimal.
From this theorem, we also get the following corollary.

\begin{corollary}
    We have $\text{\upshape \acs{PSPACE}} = \text{\upshape \acs{NPSPACE}}$.
\end{corollary}

\begin{proof}
    \begin{align*}
        \text{\acs{NPSPACE}} &= \bigcup_{k \in \mathbb{N}}\text{\acs{NSPACE}}[n^k] & \text{by definition} \\
        &\subseteq \bigcup_{k\in \mathbb{N}}\text{\acs{DSPACE}}[n^{2k}] & \text{by Savitch's Theorem} \\
        &\subseteq \bigcup_{k\in \mathbb{N}}\text{\acs{DSPACE}}[n^{k}] \\
        &= \text{\acs{PSPACE}} & \text{by definition} \\
        &= \bigcup_{k\in \mathbb{N}}\text{\acs{DSPACE}}[n^{k}] \\
        &\subseteq \bigcup_{k\in \mathbb{N}}\text{\acs{NSPACE}}[n^{k}] \\
        &= \text{\acs{NPSPACE}}
    \end{align*}
\end{proof}


\section{Results concerning the Chomsky Hierarchy}\label{sec:results-concerning-the-chomsky-hierarchy}

Now that we have seen most of the required theory, we can start to apply it to the main subject of this document: the Chomsky hierarchy.

The definitions of first- and second-order logic can be found in \cref{sec:first-order-logic} and \cref{sec:second-order-logic} respectively.
For this section, we define the vocabulary on strings to be $\sigma = \langle Q_a, Q_b, \dots, Q_z, \leq , 0, 1, \max \rangle$.
We have a unary predicate $Q_a$ for each character $a$ in $\Sigma$, a total ordering on the universe, and the constants $0, 1$ and $\max$.
For a universe $| \mathcal{A} | = \{0, 1, \dots, n - 1\}$, we require that $\max = n - 1$.
Also, for each $a \in \Sigma$ we require that $Q_a(x)$ is true if and only if the $x^{th}$ character of the string is $a$.

Only the results for context-sensitive languages are discussed in this section.
Further results can be found in \cref{sec:descriptive-complexity-context}.

\subsection{Context-Sensitive Languages}\label{subsec:des-context-sensitive-languages}

This is the language class that interests us most, as it has been studied less extensively than other language classes.
Nevertheless, there are some known formalisms.
One of them is the linear bounded nondeterministic Turing machine.
This first equivalence is shown in \cref{subsec:des-context-sensitive-languages-context-app}.

We define \acs{MSO}(\acs{TC}) to be second-order logic restricted to quantification over unary relations supplemented with the transitive closure operator.
Then, the second formalism is given by the logic \acs{MSO}(\acs{TC}).
The equivalence was established by Immerman in \cite{Immerman1987}.
The transitive closure operator takes the transitive closure over some graph and is defined as follows:

\begin{define}[Transitive closure]
    Let $\phi\left(\overline{a}, \overline{b}\right)$ be a formula with $2k$ free variables.
    We can see this formula as a directed edge relation over the graph with vertices $\overline{c} \in | \mathcal{A} |^{k}$.
    Then, the transitive closure $\left(TC_{\overline{a}, \overline{b}}\phi\left(\overline{a}, \overline{b}\right)\right)\left(\overline{u}, \overline{v}\right)$ is true if and only if there is a path in the graph generated by $\phi$ from $\overline{u}$ to $\overline{v}$.
\end{define}

\begin{theorem}
    \label{thm:contextsensitveMSOTC}
    A language $L$ is context-sensitive if and only if it can be described by a formula in $\acs{MSO}(\acs{TC})$.
\end{theorem}

\begin{proof}
    By~\cref{thm:nspacecontextsensitive}, we can also show the equivalence of \acs{MSO}(\acs{TC}) and \acs{NSPACE}$[n]$ Turing machines.

    First, we show that any formula in \acs{MSO}(\acs{TC}) can be evaluated by a \acs{NSPACE}$[n]$ Turing machine $N$.
    The first step is to notice that any relation of \acs{MSO} can be represented on the tape in $\mathcal{O}(n)$ space.
    For any sub-formula of the form $\left(TC_{\overline{a}, \overline{b}}\phi\left(\overline{a}, \overline{b}\right)\right)\left(\overline{u}, \overline{v}\right)$, $N$ writes down $\overline{u}$ and guesses the next vertex on the tape.
    Then, $N$ can check if the transition is valid by evaluating $\phi$.
    If it is, $N$ replaces $\overline{u}$ with the new vertex and repeats until it reaches $\overline{v}$.
    Because Immerman showed that \acs{NSPACE} is closed under complementation in~\cite{Immerman1988}, we know that $N$ can also compute any sub-formula of the form $\neg\left(TC_{\overline{a}, \overline{b}}\phi\left(\overline{a}, \overline{b}\right)\right)\left(\overline{u}, \overline{v}\right)$.
    The other parts of the formula can be evaluated easily in linear space, as $N$ just needs to write down relations when quantifying and remember in which part of the constant-size formula it is.

    \vspace{5mm}

    For proving the other direction, consider a \acs{NSPACE}$[n]$ Turing machine $N$.
    We define the string vocabulary for our logic to be over the alphabet $\Sigma$ which contains the symbol $b_{a_i, q_j}$ for all pairs of tape symbols and Turing machine states in addition to all normal symbols $a_i$.
    We use these symbols to completely represent an instantaneous configuration of the computation of $N$.
    This is done by having creating a string from the tape content and replacing the character at the position of the read/write head by the $b_{a_i, q_j}$ which contains the actual character and state of $N$.

    Consider the tuple $\overline{X} = \langle Q_{b_1}, \dots, Q_{b_r} \rangle$ containing a unary relation for each symbol in $\Sigma$.
    This tuple completely represents an instantaneous configuration of the tape and the Turing machine.
    We can now write a formula $\varphi(\overline{X}, \overline{Y})$ which is true if and only if a transition from state $\overline{X}$ to $\overline{Y}$ is possible in $N$.
    This can be done by a big disjunction over all rules of $N$.
    After one step of $N$, the new character $b$ at position $x$ is only determined by the actual character at position $x$ and the two characters left and right of it\footnote{we include markers at the two ends of the tape, which makes this well-defined}.
    This means that we can write all transitions of $N$ in the form $\langle b_{k},b_{l},b_{m}\rangle \to \langle b_{i}, b_{j}, b_{w} \rangle$.
    If $P$ is the set of all transitions, we have
    \begin{multline*}
        \varphi(\overline{X}, \overline{Y}) \equiv \exists i \left(\forall j \left(|i - j| > 1 \to \bigwedge_{b_i}\left(Y_{Q_{b_i}}(j) \leftrightarrow X_{Q_{b_i}}(j)\right)\right) \land \right. \\
        \left. \bigvee_{\langle b_{k},b_{l},b_{m}\rangle \to \langle b_{u}, b_{v}, b_{w} \rangle \in P} \left(X_{Q_{b_{k}}}(i - 1) \land X_{Q_{b_{l}}}(i) \land X_{Q_{b_{m}}}(i + 1) \land Y_{Q_{b_{u}}}(i - 1) \land Y_{Q_{b_{v}}}(i) \land Y_{Q_{b_{w}}}(i + 1) \right)\right)
    \end{multline*}
    The first line asserts that apart from the indices where the head is, the tape stays unchanged.
    The second line tells states that there exists a rule in $P$ such that the characters in $\overline{Y}$ at positions $i - 1, i, i + 1$ follow from the previous characters in $\overline{X}$.

    Now, we can take the transitive closure over $\varphi$, starting with the start configuration and ending at an accepting configuration.
    Without loss of generality, we can assume that $N$ clears its tape after accepting, therefore the end position is unique.
    Thus, our formula holds if and only if there is an accepting path in $N$ on the input word $w$.

    This concludes the equivalence of \acs{MSO}(\acs{TC}) and context-sensitive languages.
\end{proof}

An interesting normal form for \acs{MSO}(\acs{TC}) can be derived from this proof.
\begin{corollary}
    \label{cor:normalmso}
    Every formula in \acs{MSO}(\acs{TC}) can be written in the form
    \[
        \left( TC_{\overline{X}, \overline{Y}} \varphi(\overline{X}, \overline{Y}) \right)\left( \overline{U}, \overline{V} \right)
    \]
\end{corollary}

\begin{proof}
    We have given an explicit way to convert any formula into a Turing machine and back in \cref{thm:contextsensitveMSOTC}.
    By construction, this always gives us a formula of the required form.
\end{proof}

Returning to our research question, we now have one characterization of context-sensitive languages, but using second-order logic.
Still, we can say that these proofs show that context-sensitive languages are tightly connected to the connectivity of graphs with $2^{cn}$ vertices.
This is because the transitive closure answers exactly all questions regarding the existence of paths between two vertices in a graph, and \cref{cor:normalmso} combined with \cref{thm:contextsensitveMSOTC} shows that all formulas capturing a context-sensitive language can be written as a formula checking such a path on a graph.

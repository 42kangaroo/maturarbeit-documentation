%! suppress = MissingImport
\chapter{Descriptive Complexity}\label{ch:descriptive-complexity}


\section{Aims}\label{sec:aims}

In mathematics, abstraction is one of the most important tools as it enables us to make general statements and prove them for all the concrete instantiations of a concept.
Formal Logic takes this even further and makes it possible to abstract mathematical thought itself.
In Computer science, we are often interested in the amount of resources needed to compute a certain function or solve a certain problem, speaking in terms of time and storage space.
Different forms of logic have the power to describe different types of problems.
By focusing on decision problems\footnote{Any problem can be reduced to boolean queries, for example by having a boolean querry meaning "the i$^{th}$ bit of an encoding of the answer is 1"}, we can say a corresponding logical characterisation of a problem is a formula $\varphi$ which is true if and only if a structure satisfies the required properties.
By looking at the complexity of formulas which are needed to describe problems in terms of relations, operators, variables and other metrics, we can often find remarkably natural classes of logic corresponding to classes of problems.

Using these results, many insights into the underlying structure of real-world problems can be made which in turn can give us better ways to deal with them.
Further, descriptive complexity has applications in database theory and computer aided verification and proofs.


\section{Tools}\label{sec:tools}

As always, we first need to present some tools and techniques which will be used later in the proofs.
Definition are again taken and modified from~\cite{theory-cs} and~\cite{descriptive-complexity}.

\subsection{Complexity Theory}\label{subsec:complexity-theory}

Complexity theory is the study of the resources, measured mostly in time and space, needed to compute certain problems\footnote{The specific model of computation is not important as all give almost the same results. We will assume turing machines}.
Also, we do not really care about constants in the computation, and thus use a notation which omits these.

\begin{define}[Big-O notation]
    Let $f, g$ be functions $f, g: \mathbb{N} \to \mathbb{R}_+$.

    We say that $f \in \mathcal{O}(g)$ if there exists positive integers $n_0, c$ such that for all $n \geq n_0$ we have \[f(n) \leq c\cdot g(n)\]
\end{define}

Complexity classes can then be defined as all the problems which have a turing machine satisfying some bounds that can compute their solutions.
Now we will define some common and important complexity classes.

\begin{define}
    [{DTIME[$\mathcal{O}(t)$]}]
    We say that a decision problem is in DTIME[$\mathcal{O}(t)$] if there exists a deterministic turing machine that takes a maximum of $f(n)$ steps on any input of size $n$ and $f \in \mathcal{O}(t)$.
\end{define}

\begin{define}[P]
    We say that a decision problem is in P if there exists a polynomial $q$ such that the problem is in DTIME[$\mathcal{O}(q)$]
\end{define}

\begin{define}
    [{DSPACE[$\mathcal{O}(t)$]}]
    We say that a decision problem is in DTIME[$\mathcal{O}(t)$] if there exists a deterministic turing machine that visits a maximum of $f(n)$ tape cells on any input of size $n$ and $f \in \mathcal{O}(t)$.
\end{define}

\begin{define}[PSPACE]
    We say that a decision problem is in PSPACE if there exists a polynomial $q$ such that the problem is in DSPACE[$\mathcal{O}(q)$]
\end{define}

We can go do the same for nondeterministic turing machines, and get the corresponding complexity classes NTIME, NP, NSPACE, NPSPACE.
There, we always take the maximum of tape cells and steps over any computation branch.

The complexity class P has a special meaning for computer scientists as these are the problems which are deemed "feasible" on modern computers.

\subsection{Reduction and Completeness}\label{subsec:reduction}

A reduction can informally be seen as a method of using a problem we already solved to solve a new problem by converting this new problem into an instance of the old problem.
These reduction can be very useful to define complete problems for complexity classes, which in turn enable us to prove theorems for all problems of a specific complexity class.

\begin{define}[first-order reduction]
    Let $\mathcal{C}$ be a complexity class and $A$ and $B$ be two problems over vocabularies $\sigma$ and $\tau$.
    Now suppose that there is some first-order query $I: \text{STRUC}[\sigma] \to \text{STRUC}[\tau]$ for which we have the following property:
    \[
        \mathcal{A} \in A \Leftrightarrow I(\mathcal{A}) \in B
    \]
    Then $I$ is a first order reduction from $A$ to $B$, denoted as $A \leq_{fo} B$.
\end{define}

First order reductions can then be used to show that some problem is also a member in some complexity class, as in most complexity classes, we can compute the first order query, and then we are left with a problem that we already know is in the required class.
The converse can also be shown: for some problem $B$ which is not in some complexity class $\mathcal{C}$, if we have $B \leq_{fo} A$, then $A$ is also not in $\mathcal{C}$, as otherwise $B$ would also be in $\mathcal{C}$, which is a contradiction.

Using the reductions, we can define completeness.

\begin{define}[Completness via first-order reductions for Complexity Class $\mathcal{C}$]
    We say some problem $A$ is complete for $\mathcal{C}$ via $\leq_{fo}$ if and only if
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item $A \in \mathcal{C}$
        \item for all $B \in \mathcal{C}$, we have $B \leq_{fo} A$
    \end{itemize}
\end{define}

Informally, a complete problem captures the essence of the complexity class.
Further, they have an application in some proofs of equivalences between complexity classes $\mathcal{C}$ and logics $\mathcal{L}$.
These proofs follow the following steps as in~\cite{descriptive-complexity}:
\begin{enumerate}
    \item Show that $\mathcal{L} \subseteq \mathcal{C}$ by providing a way to convert any formula $\varphi \in \mathcal{L}$ into an algorithm in $\mathcal{C}$.
    \item Find a complete problem $T$ for $\mathcal{C}$ via first-order reductions.
    \item Show that $\mathcal{L}$ is closed under first-order reductions, that is that any formula can be extended by first-order quantifiers and boolean connectives and stay in $\mathcal{L}$.
    \item Find a formula for $T$ in $\mathcal{L}$, which shows $T \in \mathcal{L}$.
\end{enumerate}
The above steps work, as for any problem $B$ in $\mathcal{C}$, there is a first-order reduction $I$ to $T$, and both $\mathcal{L}$ and $\mathcal{C}$ are complete via these reductions, so we also have $B \in \mathcal{L} = \mathcal{C}$.

\subsection{Ehrenfeucht-Fraïssé Games}\label{subsec:ehrenfeucht-fraisse-games}

Ehrenfeucht-Fraïssé games are combinatorial games which are equivalent to first-order formulas and their extensions.
Using these games, it is often possible to show inexpressibility results for certain problems in some logic $\mathcal{L}$.

As a motivation, we can look at what it means for a formula to hold on some structure.
Assume the formula has the form $\forall x\varphi(x)$.
Then this can be seen as some opponent choosing some element $a \in |\mathcal{A}|$ and us now needing to show that $\varphi(a)$ holds.
The case where the formula has the form $\exists x\psi(x)$ can be treated similarly, but we can choose the element ourselves.

Now for the formal definition
\begin{define}[Ehrenfeucht-Fraïssé Game]
    The \emph{$k$-pebble Ehrenfeucht-Fraïssé Game $\mathcal{G}_k$} is played by two players: the Spoiler and the Duplicator on a pair of structures $\mathcal{A}$ and $\mathcal{B}$ using $k$ pairs of pebbles.
    In each move, the spoiler places one of the remaining pebbles on an element of one of the two structures.
    Then, the duplicator tries to match the move on the other structure by placing the corresponding pebble on an element.
    We say that the duplicator wins the $k$-pebble Ehrenfeucht-Fraïssé Game on $\mathcal{A}, \mathcal{B}$ if after the $k$ rounds, the map $i : |\mathcal{A}| \to |\mathcal{B}|$ defined as for all elements of $|\mathcal{A}|$ with a pebble and the constants as the element in $|\mathcal{B}|$ with the corresponding pebble or constant forms a partial isomorphism.
    A partial isomorphism is an isomorphism formed for some subset of the universe, with all relations restricted to that subset.
\end{define}

In this context, the spoiler wants to show that $\mathcal{A}$ and $\mathcal{B}$ are different, whereas the duplicator wants to show their equivalence.

As this is a zero-sum game of full information, one of the two players must have a winning strategy.
It can be proven that if the duplicator has a winning strategy for the $k$-pebble Ehrenfeucht-Fraïssé on $\mathcal{A}$ and $\mathcal{B}$ if and only if $\mathcal{A}$ and $\mathcal{B}$ agree on all formulas with less or equal to $k$ nested quantifiers.
We can use these facts to prove inexpressibility of some problems in first-order logic by exhibiting two structures $\mathcal{A}_k$ and $\mathcal{B}_k$ for each $k$ with one satisfying the problem constraints and the other not and a winning strategy for the duplicator on these two structures.
This methodology can be extended to other logics by adding new moves or restrictions to the game.

\section{Important Results}\label{sec:important-results}

\subsection{NSPACE[$s(n)$] $\subseteq$ DSPACE[$s(n)^2$]}\label{subsec:nspacesubsetdspacesquared}

This result is part of Savitch's Theorem, which introduces alternating turing machines in an intermediate step.
These TMs are a generalisation of nondeterministic TMs, which can be seen as machine taking the "or" of all it's computation paths.

\begin{define}[Alternating Turing Machine]
    An alternating Turing machine is a turing machine with two types of states: the existential and universal gates.
    Now, the acceptance conditions change compared to a NTM and depends on the state we are currently in.
    If we are in an existential state, we accept if and only if \emph{at least one} of the computations leading on from this configuration is accepting.
    If we are in an universal state, we accept if and only \emph{all} the computations leading on from this configuration are accepting.
\end{define}

These ATMs are now capable of also taking the "and" of the child states and maintain the ability to take the "or" of its children.

We define ATIME[$\mathcal{O}(t)$] and ASPACE[$\mathcal{O}(t)$] analogously to NTIME[$\mathcal{O}(t)$] and NSPACE[$\mathcal{O}(t)$].

Now we can proceed to the (quite technical) proof of Savitch's Theorem as presented in~\cite{descriptive-complexity}.

\begin{theorem}[Savitch's Theorem]
    For all space-constructible functions $t \geq \log n$ we have
    \[
        \text{NSPACE}[\mathcal{O}(t)] \subseteq \text{ATIME}[\mathcal{O}(t^2)] \subseteq \text{DSPACE}[\mathcal{O}(t^2)]
    \]
\end{theorem}

\begin{proof}
    We start with the first inclusion, $\text{NSPACE[$\mathcal{O}(t)$]} \subseteq \text{ATIME[$\mathcal{O}(t^2)$]}$.
    We now need to show that any NSPACE[$\mathcal{O}(t)$] turing machine can be simulated by a ATIME[$\mathcal{O}(t^2)$] alternating turing machine.
    Let $N$ be a NSPACE[$\mathcal{O}(t)$] turing machine.
    Without loss of generality, we assume that $N$ clears its tape after accepting and goes back to the first cell.

    Now consider $G_w$, the computation graph of $N$ on input $w$.
    We now see that $N$ accepts $w$ if and only if there is a path from the start configuration $s$ to the accepting configuration $t$.
    We now present a routine $P(d, x, y)$ which asserts that there is a path of length at most $2^{d}$ from vertex $x$ to $y$.
    Inductively, we can define $P$ as follows:
    \[
        P(d, x, y) = (\exists z)(P(d - 1, x, z) \land P(d - 1, z, y))
    \]
    This formula asserts that there exists a middle vertex $z$ for which there is a $2^{d - 1}$ path from $x$ to $z$ and from $z$ to $y$.
    Using an alternating turing machine, we can evaluate the formula using an existential state to find the middle vertex $z$, and then an universal state covering both shorter paths.

    Now for the runtime analysis, we see that we need $\mathcal{O}(t(n))$ time to write down the middle vertex $z$, as a configuration includes the tape, which has length $\mathcal{O}(t(n))$.
    Further, we then need to evaluate some $P(d - 1, z, y)$.
    By induction, we find that we need $\mathcal{O}(d\cdot t(n))$ time to compute $P(d, x, y)$.
    By the fact that there are only $2^{\mathcal{O}(t(n))}$ possible configurations, we get that the initial $d$ is also in $\mathcal{O}(t(n))$, and thus our total runtime is $\mathcal{O}(t(n)\cdot t(n)) = \mathcal{O}(t(n)^2)$.

    For the second inclusion, we need to simulate a ATIME[$\mathcal{O}(s(n))$] machine $A$ using a DSPACE[$\mathcal{O}(s(n))$] machine (here, we substituted $s(n)$ for $t(n)^2$).
    Again, we consider the computation graph of $A$ on input $w$.
    This graph has depth $\mathcal{O}(s(n))$ and size $2^{\mathcal{O}(s(n))}$.

    We can systematically search this computation graph to get our answer.
    This is done by keeping a string of choices $c_{1}c_{2}\dots c_r$ of length $\mathcal{O}(s(n))$ made until this point.
    Note that this uniquely determines which state we are in.

    Now, we can find the answer recursively.
    If we are in a halt state, we report this back to the previous state.
    In an existential state, we simulate its children, and if we get a positive result from one of them, we also return a positive result.
    In an universal state, we simulate its children, and if we get a positive result from all of them, we return a positive result.

    In total, we use only $\mathcal{O}(s(n))$ space, to simulate $A$.

    Thus, the second part of the theorem follows and by transitivity of $\subseteq$ we have $\text{NSPACE}[\mathcal{O}(t(n))] \subseteq \text{DSPACE}[\mathcal{O}(t(n)^2)]$.
\end{proof}

We do not know if the containment is strict or not for any of the inclusions of the theorem.
From this theorem, we also get the following interesting corollary.

\begin{corollary}
    We have $\text{PSPACE} = \text{NPSPACE}$.
\end{corollary}

\begin{proof}
    \begin{align*}
        \text{NPSPACE} &= \bigcup_{k \in \mathbb{N}}^{\infty}\text{NTIME}[\mathcal{O}(n^k)] \\
        &\subseteq \bigcup_{k\in \mathbb{N}}^{\infty}\text{DTIME}[\mathcal{O}(n^{2k})] \\
        &= \bigcup_{k\in \mathbb{N}}^{\infty}\text{DTIME}[\mathcal{O}(n^{k})] \\
        &= \text{PSPACE} \\
        &\subseteq \bigcup_{k\in \mathbb{N}}^{\infty}\text{NTIME}[\mathcal{O}(n^{k})] \\
        &= \text{NPSPACE}
    \end{align*}
\end{proof}

\subsection{SPACE Hierarchy theorem}\label{subsec:space-hierarchy-theorem}

The SPACE hierarchy theorem states that for both nondeterministic and deterministic space, we have problems that can be solved in some space $t(n)$, but not in less.
Formally, we have
\[
    \text{DSPACE}[o(t)] \subsetneq \text{DSPACE}[\mathcal{O}(t)]
\]
where $o(t)$ is the set of functions $f$ such that $f \in \mathcal{O}(t)$ but $t \not \in \mathcal{O}(f)$, that is all functions that grow more slowly than $t$.
This holds for all space-constructible $t \geq \log n$.
The same holds for NSPACE.

We will present a proof for deterministic space.
\begin{proof}
    The proof uses a diagonalization argument by presenting some machine $D$ that takes a turing machine $M$ and an input size in unary as input and does the opposite of $M$ if it halts.
    We want to show that for all $M$ which run in space $f(n) \in o(t(n))$, we have an input on which $D$ and $M$ do not agree.
    This would show that the language computed by $D$ is not in DSPACE[$o(t)$], and thus the strict containment.

    On input $\langle M, 1^{k} \rangle$ our machine $D$ marks of $t(|\langle M, 1^{k} \rangle|)$ tape cells, which are the cells that are allowed for the computation.
    Further we also maintain a counter with size $|M|\cdot 2^{t(|\langle M, 1^{k} \rangle|)}$, which is the maximum amount of different configurations a TM can pass before looping on a binary tape of size $t(|\langle M, 1^{k} \rangle|)$.
    Then, we simulate $M$ on input $\langle M, 1^{k} \rangle$.
    If we transcend any bound, we reject.
    For all $M$ in DSPACE[$o(t)$], there is a $k$ such that $f(n) \leq t(n)$ by definition.
    On this input, the simulation finishes, and we can invert the output.

    This directly gives us an input for which $M$ and $D$ differ, and thus proves our claim.
    Furthermore, $D$ runs in DSPACE[$\mathcal{O}(t)$] as by construction we assured that we do not run infinitely and that we stay within the space bound.
\end{proof}


\section{Results concerning the Chomsky hierarchy}\label{sec:results-concerning-the-chomsky-hierarchy}

Now that we have seen most of the required theory, we can start to apply it to the main theme of this work, the Chomsky hierarchy.
For this section, we define the vocabulary on strings to be $\sigma = \langle \{0, \dots, n - 1\}, Q_a, Q_b, \dots, Q_z, \leq , 0, 1, \text{max} \rangle$.
The universe consists of the numbers from $0$ to $n - 1$, we have a unary predicate for each character in $\Sigma$, a total ordering on the universe, and the constants $0, 1$ and $\text{max} = n - 1$.


\subsection{Regular Languages}\label{subsec:des-regular-languages}

Here, we will show that the regular languages are captured exactly by second-order logic where we restrict ourselves to quantify only over predicates of arity one and do not include $\leq$.
Further, we also are not allowed to use $\leq$, but have access to equality $x = y$ and the successor relation $x = y + 1$.
We call this class SOM[$+1$].

First we need to present a formal definition of deterministic finite automata.
\begin{define}[DFA]
    A deterministic finite automaton is a 5-tuple $M = \langle Q, \Sigma, \delta, q_0,  F \rangle$ where
    \begin{itemize}
        \item[$Q$] is the set of states
        \item[$\Sigma$] is the alphabet
        \item[$\delta$] is the transition function mapping a state and a symbol to the next state, so formally $\delta : Q\times \Sigma \to Q$.
        \item[$q_0$] the start state
        \item[$F$] a subset of $Q$ which are the accepting states.
    \end{itemize}
\end{define}
We say that a DFA $D$ accepts a word $w \in \Sigma^{*}$ if when starting at the start state, if we go through $w$ and always transition to the next state according to the actual symbol in $w$ and the actual state, we end up in an accepting state.

In~\cite{theory-cs} and~\cite{Straubing1994} there is a proof of the following fact we will use in our proof for SOM[$+1$]:
\begin{theorem}
    For any alphabet $\Sigma$, there is a DFA recognising language $L \subseteq \Sigma^{*}$ if and only if it is regular.
\end{theorem}

Now we can start to prove our main theorem for regular languages.
\begin{theorem}
    For any alphabet $\Sigma$, a language $L \subseteq \Sigma^{*}$ is expressible in SOM[$+1$] if and only if it is regular.
\end{theorem}

\begin{proof}
    First we show that any regular language can be expressed in SOM[$+1$].
    Let $L$ be regular, end $D_L$ be a DFA recognising the language.
    We assume $L$ does not contain the empty word, otherwise we can recognise the language $L \setminus \{\varepsilon\}$ and then add $\varphi \lor \forall x(x \neq x)$, which adds the empty string back.

    Now let $D_L$ have $k$ states.
    We can existentially quantify unary relations $X_1, \dots, X_k$ to have the meaning that $X_i(y)$ is true if and only if $D_L$ is in state $i$ after $y$ steps.
    Then, we need to make consistency checks.
    We present formulas for each of the consistency checks, and then can take the "and" of those to get our final formula $\exists X_1, \dots, X_k(\varphi_1 \land \varphi_2 \land \varphi_3)$.
    \begin{description}
        \item[The start state is $q_j$] we have \[\varphi_1 \coloneqq \bigwedge_{i = 1}^{k} (i = j \leftrightarrow X_j(0))\]
        \item[We end in an accepting state] Let $T_i$ be the set of all characters which lead from $q_i$ to an accepting state.
        Then we have
        \[
            \varphi_2 \coloneqq \bigwedge_{i = 1}^{k}\left(X_i(\text{max}) \to \bigvee_{a \in T_i} Q_a(max)\right)
        \]
        \item[We move according to the transition function] We have
        \begin{align*}
            \forall x\left( \forall y \left( y = x + 1 \to \left(\bigwedge_{i = 1}^{k} \bigwedge_{a \in \Sigma} \left(\left(X_i(x) \land Q_a(x)\right) \to X_{\delta(i, a)}(y)\right) \right. \right. \right. \\
            \left. \left. \left.\land \bigwedge_{i = 1}^{k}\bigwedge_{a \in \Sigma} \left(\left(X_i(y) \land Q_a(x)\right) \to \bigvee_{r = 1}^{k}\left(X_r(x) \land \delta(r, a) = j\right)\right)  \right)  \right) \right)
        \end{align*}
    \end{description}
    By induction we can show that always exactly one $i$ satisfies $X_i(x)$ for any $x$.
    Thus, if the created formula is satisfied, we know that $D_L$ accepts the word, and thus we have described $L$ in SOM[$+1$].

    For the other direction, we need to introduce two new concepts.

    One of them is the nondeterministic finite automaton, which is analogous to the nondeterministic turing machine as it can also have multiple transitions going from the same state.
    As with the NTM and the TM, both the DFA and the NFA have the same expressive power.

    The other concept is that of $(\mathcal{V}_1, \mathcal{V}_2)$-structures.
    These structures are generalisations of our former vocabulary $\sigma$ as they have characters in $A \times \mathcal{P}(\mathcal{V}_1)\times \mathcal{P}(\mathcal{V}_2)$.
    These structures are useful as we can make $\mathcal{V}_1$ to be the set of free first-order variables in a formula $\varphi$ and $\mathcal{V}_2$ be the set of free second-order variables in the formula.
    If at a position $i$ in our $(\mathcal{V}_1, \mathcal{V}_2)$-structures we have $x$ in the first-order component of its character, we see this as meaning that $x = i$.
    For the second-order variables in the third component, a $X$ at position $i$ means that $X(i)$ holds.

    Now, we can prove by induction that all formulas in SOM[$+1$] with free variables in $\mathcal{V}_1$ and $\mathcal{V}_2$ are regular.
    Sentences, the formulas without free variables are the special case where $\mathcal{V}_1, \mathcal{V}_2 = \emptyset$.

    First, we need to check that the $(\mathcal{V}_1, \mathcal{V}_2)$-structures are consistent, and no first-order variable $x$ appears more than once.
    This can be done by a NFA which has one state for each subset of variables, and extends its subset while going over the string.
    If a variable appears twice, we enter a state that always loops and rejects.

    Then, we see that the atomic formulas can be checked, as $x = y$, $x = y + 1$ and $Q_a(x)$ are easy to check, and checking $X(x)$ is equivalent to looking if the occurrence of $x$ has $X$ in the third component.
    We always need to take the intersection with the NFA which checks if the structure is valid.

    All boolean connective are also valid as regular languages are closed under complement, intersection and union as seen in~\cite{theory-cs}.

    The most difficult case is a formula of the form $\exists x \varphi$ (as $\forall x \varphi \equiv \neg \exists x \neg \varphi)$).
    If $\exists x \varphi$ is over $(\mathcal{V}_1, \mathcal{V}_2)$-structures, then $\varphi$ is over $(\mathcal{V}_1 \cup \{x\}, \mathcal{V}_2)$-structures.
    By induction, we know that $\varphi$ defines a regular language and thus there is a NFA $N$ which recognises it.
    For the new automaton, we duplicate our states, with the meanings "used $x$" and "not used $x$".
    If we are in a state where $x$ was used, we can not take any transition with $x$ in the second set.
    If we are in a state where $x$ was not used, we can take a transition with $x$ in the second set and go to the corresponding state with $x$ used or take a transition where $x$ is not used and go to the corresponding state where $x$ was not used.

    The remaining case with second-order variables is treated analogously, without the restriction on the number of times the variable is used, so we do not need to duplicate our states.

    By induction, we have thus showed the other direction, and we see that SOM[$+ 1$] and the regular languages are equivalent.
\end{proof}

\subsection{Context-Free Languages}\label{subsec:des-context-free-languages}

\subsection{Context-Sensitive Languages}\label{subsec:des-context-sensitive-languages}

\subsection{Recursive Languages}\label{subsec:des-recursive-languages}


\section{Open questions}\label{sec:open-questions}

The domain of descriptive complexity is full of open questions as the proofs of lower bounds seems to be very difficult in most cases.
Further, even separation between complexity classes wich seem to take an exponential amount of resources compared to another one in practice can not be shown to be different.

\subsection{P$\overset{?}{=}$NP}\label{subsec:pnp}
The P vs. NP question is the most emblematic question in descriptive complexity theory.
In practice, for any NP-complete problem, only exponential worst-case algorithms are known.
This leads to the widely believed conjuncture that P $\neq$ NP.
The problem is one of the seven Millennium Problems and a solution of equality or inequality is worth 1 Million US dollars.

The consequences of a solution stating that P $=$ NP could have many practical advantages if it was constructive and had a low constant, as many important problems in research and logistics could be solved quickly.
It would also mean the breakdown of most of modern cryptography, which relies on problem being intractable.
On a conceptual level, it would mean that finding a proof to a problem is not harder than verifying its correctness, which would greatly impact the work of mathematicians.
If a proof of the contrary would be known, this would focus the research more on the average case complexity of NP problems, but because of the continued lack of success on the question, this shift has already widely taken place.

\subsection{NSPACE[$O(n)$]$\overset{?}{=}$DSPACE[$O(n)$]}\label{subsec:nspacedspace}

This problem is known under the name first Linear bounded automaton problem since its proposal by Kuroda in~\cite{Kuroda1964}, and asks if nondeterminism adds power in the context of bounded space.
This comes from the fact that a NSPACE[$\mathcal{O}(n)$] turing machine can be seen as a TM with a linear bound on its space usage.
This theorem is of interest as we know that NSPACE[$\mathcal{O}(n)$] is equivalent to the context-sensitive languages by~\cref{subsec:des-context-sensitive-languages}.

Since the proposal, there were two advances.
One is the proof that NSPACE is closed under complement.
The contrary would have implied $\text{NSPACE}[\mathcal{O}(n)] \neq \text{DSPACE}[\mathcal{O}(n)]$ as DSPACE is closed under complement.
The second advance is Savitch's Theorem in~\cref{subsec:nspacesubsetdspacesquared} which already gives a bound for simulating NSPACE using DSPACE machines.
It is not known if this theorem is optimal, that is whether the blowup by a power of 2 is optimal or if we can do better.

An equality would imply that the context-sensitive languages can be recognised by a deterministic linear bounded automaton, which could make recognising words in context-sensitive langauges easier and faster.
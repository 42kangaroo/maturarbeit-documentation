%! suppress = UnresolvedReference
%! suppress = MissingImport


\chapter{Descriptive Complexity}\label{ch:descriptive-complexity}


\section{Aims}\label{sec:aims}

In mathematics, abstraction is one of the most important tools, as it enables us to make general statements and prove them for all the concrete instances of a concept.
Formal logic takes this even further and makes it possible to abstract mathematical thought itself.
In computer science, we are always interested in the amount of resources needed to compute a certain function or solve a certain problem, speaking in terms of time and storage space.
By focusing on decision problems\footnote{Any problem can be reduced to boolean queries, for example by having a boolean query meaning ``the i$^{th}$ bit of an encoding of the answer is 1''}, we can define a logical characterization of a problem as a formula $\varphi$ which is true if and only if a structure satisfies the required properties.
Different forms of logic have the power to describe different classes of problems.
By looking at the complexity of formulas which are needed to describe problems in terms of relations, operators, variables and other metrics, we can often find remarkably natural classes of logic corresponding to classes of problems.

Using these results, many insights into the underlying structure of real-world problems can be made, which in turn can give us better ways to tackle them.
Further, Descriptive Complexity has applications in database theory, computer aided verification, and proofs.


\section{Tools}\label{sec:tools}

As always, we first need to present some tools and techniques which will be used later for proofs.
In this section, we will only present Complexity theory.
In \cref{subsec:ehrenfeucht-fraisse-games} and \cref{subsec:reduction}, we also present Ehrenfeucht-Fraïssé games and first-order reductions.
These tools are not needed for \cref{ch:personal-contribution}, but using them was tried extensively during the research phase.
Definition are again taken from~\cite{theory-cs} and~\cite{descriptive-complexity}.

\subsection{Complexity Theory}\label{subsec:complexity-theory}

Complexity theory is the study of the resources, measured mostly in time and space, needed to compute certain classes of problems\footnote{The specific model of computation is not important as all give almost the same results. We will assume Turing machines.}.
As different computers can handle tasks at various speeds, a notation which only considers the asymptotic use of resources is used.

\begin{define}[Big-O notation]
    Let $f, g$ be functions $f, g: \mathbb{N} \to \mathbb{R}_+$.

    We say that $f(n) \in \mathcal{O}(g(n))$ if there exists positive integers $n_0, c$ such that for all $n \geq n_0$ we have \[f(n) \leq c\cdot g(n)\]
\end{define}

Complexity classes can be defined as the set of all the problems for which there exists a Turing machine satisfying some bounds that computes the answer to the problem.
Now, we will define some common complexity classes.

\begin{define}
[{DTIME$[\mathcal{O}(t(n))]$}]
    We say that a decision problem is in DTIME$[\mathcal{O}(t(n))]$ if there exists a deterministic Turing machine that takes a maximum of $f(n)$ steps on any input of size $n$ and $f(n) \in \mathcal{O}(t(n))$.
\end{define}

\begin{define}[P]
    We say that a decision problem is in P if there exists a polynomial $q$ such that the problem is in DTIME$[\mathcal{O}(q(n))]$
\end{define}

\begin{define}
[{DSPACE$[\mathcal{O}(t(n))]$}]
    We say that a decision problem is in DSPACE$[\mathcal{O}(t(n))]$ if there exists a deterministic Turing machine that visits a maximum of $f(n)$ tape cells on any input of size $n$ and $f(n) \in \mathcal{O}(t(n))$.
\end{define}

\begin{define}[PSPACE]
    We say that a decision problem is in PSPACE if there exists a polynomial $q$ such that the problem is in DSPACE$[\mathcal{O}(q(n))]$
\end{define}

We can go do the same for nondeterministic Turing machines, and get the corresponding complexity classes NTIME, NP, NSPACE, NPSPACE\@.
There, we always take the maximum of tape cells and steps over any computation branch.

The complexity class P has a special meaning for computer scientists, as the problems in P are deemed ``feasible'' on modern computers.


\section{Important Results}\label{sec:important-results}

In this section, we will only introduce Savitch's theorem.
In \cref{subsec:space-hierarchy-theorem}, we also present the space hierarchy theorem.

\subsection{Savitch's Theorem}\label{subsec:nspacesubsetdspacesquared}

Savitch's theorem relates nondeterministic space-bounded Turing machines with deterministic space-bounded Turing machines.
As an intermediate step, it introduces alternating Turing machines.
These Turing machines are a generalization of nondeterministic Turing machines, which can be seen as machines taking the ``or'' over all their computation paths.

\begin{define}[Alternating Turing Machine]
    An alternating Turing machine is a Turing machine with two types of states: existential and universal.
    The accepting conditions change compared to a nondeterministic Turing machine and depend on the current state of the machine.
    If it is in an existential state, the current configuration is accepting if and only if \emph{at least one} of the directly reachable configurations is accepting.
    If the machine is in a universal state, the current configuration is accepting if and only \emph{all} the directly reachable configurations are accepting.
\end{define}

In addition to taking the ``or'' over its children, an alternating Turing machine can also take the ``and''.

\sloppy We define ATIME$[\mathcal{O}(t(n))]$ and ASPACE$[\mathcal{O}(t(n))]$ analogously to NTIME$[\mathcal{O}(t(n))]$ and NSPACE$[\mathcal{O}(t(n))]$.

We can proceed to the (quite technical) proof of Savitch's theorem using the technique presented in~\cite{descriptive-complexity}.
\begin{theorem}[Savitch's Theorem]
    For all space constructible functions with $t(n) \geq \log n$ we have
    \[
        \text{\upshape NSPACE}[\mathcal{O}(t(n))] \subseteq \text{\upshape ATIME}[\mathcal{O}(t(n)^2)] \subseteq \text{\upshape DSPACE}[\mathcal{O}(t(n)^2)]
    \]
\end{theorem}

\begin{proof}
    We start with the first inclusion, $\text{NSPACE$[\mathcal{O}(t(n))]$} \subseteq \text{ATIME$[\mathcal{O}(t(n)^2)]$}$.
    We need to show that any NSPACE$[\mathcal{O}(t(n))]$ Turing machine can be simulated by a ATIME$[\mathcal{O}(t(n)^2)]$ alternating Turing machine.
    Let $N$ be a NSPACE$[\mathcal{O}(t(n))]$ Turing machine.
    Without loss of generality, we assume that $N$ clears its tape after accepting and goes back to the first cell.

    Consider $G_w$, the computation graph of $N$ on input $w$.
    This graph consists of configurations as vertices and directed edges from each configuration to all configurations that are directly reachable from it.
    We see that $N$ accepts $w$ if and only if there is a path from the start configuration $s$ to the accepting configuration $t$ in $G_w$.
    We now present a routine $P(d, x, y)$ which asserts that there is a path of length at most $2^{d}$ from vertex $x$ to $y$.
    Inductively, we can define $P$ as follows:
    \[
        P(d, x, y) = (\exists z)(P(d - 1, x, z) \land P(d - 1, z, y))
    \]
    This formula asserts that there exists a middle vertex $z$ for which there is a $2^{d - 1}$ path from $x$ to $z$ and from $z$ to $y$.
    The base case for $d = 0$ is to check if we can reach $x$ from $y$ by a transition in the transition table of $N$.
    Using an alternating Turing machine, we can evaluate the formula using existential states to find the middle vertex $z$, and then a universal state choosing which one of the two new paths we check.

    For the runtime analysis, we proceed as follows.
    It takes $\mathcal{O}(t(n))$ time to write down the middle vertex $z$, as each configuration includes the tape contents, which have length $\mathcal{O}(t(n))$.
    Further, we then evaluate one of the new shorter paths $P(d - 1, z, y)$.
    By induction, we find that we need $\mathcal{O}(d\cdot t(n))$ time to compute $P(d, x, y)$.
    There are only $2^{\mathcal{O}(t(n))}$ possible configurations, we get that the initial $d$ is also in $\mathcal{O}(t(n))$, and thus our total runtime is $\mathcal{O}(t(n)\cdot t(n)) = \mathcal{O}(t(n)^2)$.
    \vspace{5mm}
    \sloppy For the second inclusion, we need to simulate an ATIME$[\mathcal{O}(s(n))]$ machine $A$ using a DSPACE$[\mathcal{O}(s(n))]$ machine (here, we substituted $s(n)$ for $t(n)^2$).
    Again, we consider the computation graph of $A$ of on input $w$.
    This graph has depth $\mathcal{O}(s(n))$ and size $2^{\mathcal{O}(s(n))}$.

    We can systematically search this computation graph to get our answer.
    This is done by keeping a string of choices $c_{1}c_{2}\dots c_r$ of length $\mathcal{O}(s(n))$ made until this point.
    Each choice takes up $\mathcal{O}(1)$ place, as the number of transitions from one state is bounded by a constant depending only on $A$.
    Note that this uniquely determines which state we are in.

    Now, we can find the answer recursively.
    If we are in a halt state, we report this back to the previous state.
    In an existential state, we simulate its children, and if we get a positive result from one of them, we also report a positive result.
    In a universal state, we simulate its children, and if we get a positive result from all of them, we report a positive result.

    Overall, we use only $\mathcal{O}(s(n))$ space to simulate $A$.

    Thus, the second part of the theorem follows and by the transitivity of $\subseteq$ we have $\text{NSPACE}[\mathcal{O}(t(n))] \subseteq \text{DSPACE}[\mathcal{O}(t(n)^2)]$.
\end{proof}

We do not know if the containment is strict for any of the inclusions.
Neither is it known if the quadratic overhead for simulating nondeterministic space is optimal.
From this theorem, we also get the following interesting corollary.

\begin{corollary}
    We have $\text{\upshape PSPACE} = \text{\upshape NPSPACE}$.
\end{corollary}

\begin{proof}
    \begin{align*}
        \text{NPSPACE} &= \bigcup_{k \in \mathbb{N}}^{\infty}\text{NTIME}[\mathcal{O}(n^k)] \\
        &\subseteq \bigcup_{k\in \mathbb{N}}^{\infty}\text{DTIME}[\mathcal{O}(n^{2k})] \\
        &= \bigcup_{k\in \mathbb{N}}^{\infty}\text{DTIME}[\mathcal{O}(n^{k})] \\
        &= \text{PSPACE} \\
        &\subseteq \bigcup_{k\in \mathbb{N}}^{\infty}\text{NTIME}[\mathcal{O}(n^{k})] \\
        &= \text{NPSPACE}
    \end{align*}
\end{proof}


\section{Results concerning the Chomsky Hierarchy}\label{sec:results-concerning-the-chomsky-hierarchy}


Now that we have seen most of the required theory, we can start to apply it to the main subject of this work, the Chomsky Hierarchy.
For this section, we define the vocabulary on strings to be $\sigma = \langle Q_a, Q_b, \dots, Q_z, \leq , 0, 1, \max \rangle$.
We have a unary predicate for each character in $\Sigma$, a total ordering on the universe, and the constants $0, 1$ and $\max$.
For a universe $| \mathcal{A} | = \{0, 1, \dots, n\}$, we require that $\max = n$. Also, for each $a \in \Sigma$ we have $Q_a(x)$ if and only if the $x^{th}$ character of the string is $a$.
Again, only the results for context-sensitive languages are discussed in this section.
Further results can be found in the context appendix under \cref{sec:descriptive-complexity-context}.

\subsection{Context-Sensitive Languages}\label{subsec:des-context-sensitive-languages}

This is the language class that interests us most, as it has been studied less extensively than other language classes.
Nevertheless, there are some known formalisms.
One of them is the linear bounded nondeterministic Turing machine.
This first equivalence is shown in \cref{subsec:des-context-sensitive-languages}.
The second formalism is given by the logic MSO(TC).
This logic is defined as second-order logic restricted to quantification over unary relations supplemented with a transitive closure operator.
The transitive closure operator takes the transitive closure over some graph and is defined as follows:

\begin{define}[Transitive closure]
    Let $\phi\left(\overset{k}{a}, \overset{k}{b}\right)$ be a formula with $2k$ free variables.
    We can see this formula as a directed edge relation over the graph with vertices $\overset{k}{c} \in | \mathcal{A} |^{k}$.
    Then, the transitive closure $\left(TC_{\overset{k}{a}, \overset{k}{b}}\phi\left(\overset{k}{a}, \overset{k}{b}\right)\right)\left(\overset{k}{u}, \overset{k}{v}\right)$ is true if and only if there is a path in the graph generated by $\phi$ from $\overset{k}{u}$ to $\overset{k}{v}$.
\end{define}

\begin{theorem}
    \label{thm:contextsensitveMSOTC}
    A language $L$ is context-sensitive if and only if it can be described by a formula in $MSO(TC)$.
\end{theorem}

\begin{proof}
    By~\cref{thm:nspacecontextsensitive}, we can also show the equivalence of MSO(TC) and NSPACE$[\mathcal{O}(n)]$ Turing machines.

    First, we show that any formula in MSO(TC) can be evaluated by a NSPACE$[\mathcal{O}(n)]$ Turing machine.
    The first step is to notice that any relation of MSO can be represented on the tape in $\mathcal{O}(n)$ space.
    For any sub-formula of the form $\left(TC_{\overset{k}{a}, \overset{k}{b}}\phi\left(\overset{k}{a}, \overset{k}{b}\right)\right)\left(\overset{k}{u}, \overset{k}{v}\right)$, we write down $\overset{k}{u}$ and guess the next vertex on the tape.
    Then, we can check if the transition is valid by evaluating $\phi$.
    If it is, we replace $\overset{k}{u}$ with the new vertex and repeat until we reach $\overset{k}{v}$.
    Because Immerman showed that NSPACE is closed under complementation in~\cite{Immerman1988}, we know that we can also compute any sub-formula of the form $\neg\left(TC_{\overset{k}{a}, \overset{k}{b}}\phi\left(\overset{k}{a}, \overset{k}{b}\right)\right)\left(\overset{k}{u}, \overset{k}{v}\right)$.
    The other parts of the formula can be evaluated easily in linear space, as we just need to write down relations when quantifying and remember in which part of the constant-size formula we are.

    \vspace{5mm}

    For the other direction, consider a NSPACE$[\mathcal{O}(n)]$ Turing machine $N$.
    We define the string vocabulary for our logic to be over the alphabet $\Sigma$ which contains the symbol $b_{a_i, q_j}$ for all pairs of tape symbols and Turing machine states.

    Consider the tuple $\overline{X} = \langle Q_{b_1}, \dots, Q_{b_r} \rangle$ containing a unary relation for each symbol in $\Sigma$.
    This tuple completely represents an instantaneous configuration of the tape and the machine.
    We can now write a formula $\varphi(\overline{X}, \overline{Y})$ which means that a transition from state $\overline{X}$ to $\overline{Y}$ is possible in $N$.
    This can be done by a big disjunction over all rules of $N$.
    After one step of a Turing machine, a character $b$ at position $x$ is only determined by the actual character at position $x$ and the two characters left and right of it\footnote{we include markers at the two ends of the tape, which makes this well-defined}.
    This means that we can write all transitions of $N$ in the form $\langle b_{k},b_{l},b_{m}\rangle \to \langle b_{i}, b_{j}, b_{w} \rangle$.
    If $P$ is the set of all transitions, we have
    \begin{multline*}
        \varphi(\overline{X}, \overline{Y}) \equiv \exists i \left(\forall j \left(|i - j| > 1 \to \bigwedge_{b_i}\left(Y_{Q_{b_i}}(j) \leftrightarrow X_{Q_{b_i}}(j)\right)\right) \land \right. \\
        \left. \bigvee_{\langle b_{k},b_{l},b_{m}\rangle \to \langle b_{u}, b_{v}, b_{w} \rangle \in P} \left(X_{Q_{b_{k}}}(i - 1) \land X_{Q_{b_{l}}}(i) \land X_{Q_{b_{m}}}(i + 1) \land Y_{Q_{b_{u}}}(i - 1) \land Y_{Q_{b_{v}}}(i) \land Y_{Q_{b_{w}}}(i + 1) \right)\right)
    \end{multline*}
    The first line asserts that apart from the indices where the head is and thus where we change something, the tape stays unchanged.
    The second line tells us that there exists a rule in $P$ such that the characters in $\overline{Y}$ at positions $i - 1, i, i + 1$ follow from the previous characters in $\overline{X}$.

    Now, we can take the transitive closure over $\varphi$, starting with the start configuration and ending at an accepting position.
    Without loss of generality, we can assume that $N$ clears its tape after accepting, so the end position is unique.
    Thus, our formula holds if and only if there is an accepting path in $N$ on the input word $w$.

    This concludes the equivalence of MSO(TC) and context-sensitive languages.
\end{proof}

An interesting normal form for MSO(TC) can be derived from the proof.
\begin{corollary}
    Every formula in MSO(TC) can be written in the form
    \[
        \left( TC_{\overline{X}, \overline{Y}} \varphi(\overline{X}, \overline{Y}) \right)\left( \overline{U}, \overline{V} \right)
    \]
\end{corollary}

\begin{proof}
    We have given an explicit way to convert any formula into a Turing machine and back in \cref{thm:contextsensitveMSOTC}.
    By construction this always gives us a formula of the required form.
\end{proof}
%! suppress = UnresolvedReference
%! suppress = MissingImport
\chapter{Formal languages}\label{ch:formal-languages}


\section{Definition}\label{sec:definition}

In informatics, we often get an input as a string of characters, and want to compute some function on it.
In Complexity theory, we mostly focus on decision problems where we only want to find out if some input fulfils some given property\footnote{This can be shown to be equivalent to computing functions}.
To formalize this, there is the concept of formal languages.
The following definitions are taken from the lecture Theory of Computer Science~\cite{theory-cs}.
For the mathematical background, refer to \autoref{ch:mathematical-background}.

\begin{define}[Alphabet]
    An alphabet $\Sigma$ is a finite set of symbols
\end{define}

\begin{define}[Word]
    A word over some alphabet $\Sigma$ is a finite sequence of symbols from $\Sigma$.
    We denote $\varepsilon$ as the empty word, $\Sigma^*$ as the set of all words over $\Sigma$, $xy$ as the concatenation of $x$ and $y$, $x^{n}$ as the concatenation of $x$ with itself $n$ times, and $|w|$ as the number of symbols in $w$.
\end{define}

\begin{define}[Formal language]
    A formal language is a set of words over some alphabet $\Sigma$, or equivalently a subset of $\Sigma^*$
\end{define}

For any computational decision problem, we can reformulate it as the problem of deciding if the input word is contained in the formal language consisting of all words which have the required property.


\section{Chomsky Hierarchy}\label{sec:chromsky-hierarchy}

One of the multiple ways to categorize formal languages was invented by Avram Noam Chomsky, a modern linguist, in \cite{Chomsky1959}.
It is based on the complexity of defining the language using formal grammars, which are a finite representation of formal languages (which can be infinite in the general case).

\subsection{Grammars}\label{subsec:grammars}

A grammar can informally be seen as a set of rules telling us how to generate all words in a language.

\begin{define}[Grammar]
    A grammar is a 4-tuple $\langle V, \Sigma, R, S \rangle$ consisting of
    \begin{itemize}
        \item[$V$] The set of non-terminal symbols
        \item[$\Sigma$] The set of terminal symbols
        \item[$R$] A set of rules, formally over $(V \cup \Sigma)^{*}V(V \cup \Sigma)^{*} \times (V \cup \Sigma)^{*}$
        \item[$S$] The start symbol from the set $V$
    \end{itemize}
\end{define}

The non-terminal symbols in $V$ are symbols that are not in $\Sigma$ and exist for the purpose of steering the process of word generation.
They do not appear in any of the final words generated by the grammar.
Further, the rules dictate that there must be at least one non-terminal symbol on the left-hand side of any production rule.
This is because $(V \cup \Sigma)^{*}V(V \cup \Sigma)^{*}$ means all words having one symbol in $V$ enclosed by words in $(V \cup \Sigma)^{*}$, which are all words consisting of symbols in $V$ and $\Sigma$.
For better readability, we write rules in the form $a \to b$ instead of $\langle a, b \rangle$.

To generate the words, we have the concept of derivations.
\begin{define}[Derivation]
    First, we can define one derivation step.

    We say $u'$ can be derived from $u$ if
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item $u$ is of the form $xyz$ and $u'$ is of the form $xy'z$ for some words $x, y, y', z \in (V \cup \Sigma)^*$
        \item there exists a rule $y \to y'$ in $R$
    \end{itemize}

    We say that a word is in the \emph{generated language} of a grammar if it consists only of symbols in $\Sigma$ and can be derived in a finite number of steps from $S$.
\end{define}

\begin{exmp}
    Consider the grammar $\langle \{S\}, \{a, b\}, R, S \rangle$ with
    \[
        R = \begin{Bmatrix*}[l]
                S \to aSb,
                &S \to \varepsilon
        \end{Bmatrix*}
    \]
    The generated language for this grammar is $\{\varepsilon, ab, aabb, \dots\} = \{a^{n}b^{n} \mid n \in \mathbb{N}_0\}$
\end{exmp}

Now that we have a tool to describe some infinite languages using a finite description, we can further differentiate the complexity of a language by the minimum required complexity of the rules in any formal grammar describing that language.
In the main section, we will only present the context-sensitive languages, as they are important for later chapters.
In \cref{sec:formal-languages-app} explanations for regular languages (\cref{subsec:regular-languages}), context-free languages (\cref{subsec:context-free-languages}) and recursive languages (\cref{subsec:recursive-languages}) are provided.

\subsection{Context-Sensitive Languages}\label{subsec:context-sensitive-languages}

The most important category of languages for this work can be defined by multiple equivalent restrictions on the grammars.

One restriction is that all rules have to be of the form $\alpha\beta\gamma \to \alpha\varphi\gamma$ with $\alpha, \gamma \in (\Sigma \cup V)^{*}$, $\beta \in V$ and $\varphi \in (\Sigma \cup V)^{+}$.
This means that only the non-terminal is allowed to change.
Additionally, if $S$ is the start variable and never occurs on the right-hand side of any rule, we may include $S \to \varepsilon$.

Equivalently, we can require that for any rule $u \to v$ we have $|u| \leq |v|$.
This means that applying any rule will make the result longer.
Again, we also allow the special rule $S \to \varepsilon$ if $S$ does not occur on the right-hand side of any rule in $R$.
These grammars are called noncontracting.

The last, most useful restriction for proofs is the Kuroda normal form~\cite{Pettorossi2022}, where all rules have one of the following structures:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item $A \to BC$
    \item $AB \to CB$
    \item $A \to a$
    \item $S \to \varepsilon$ if $S$ is the start symbol and does not occur on any right-hand side of a rule
\end{itemize}
where $A, B, C, S \in V$ and $a \in \Sigma$.

\begin{exmp}
    Consider the grammar $\langle \{S, B\}, \{a, b, c\}, R, S \rangle$ with
    \[
        R = \begin{Bmatrix*}[l]
               S \to abc, &S \to aSBc, \\
               cB \to Bc, &bB \to bb
        \end{Bmatrix*}
    \]
    It generates the language $a^{n}b^{n}c^{n}$ for $n \in \mathbb{N}_{1}$ and is noncontracting.
\end{exmp}

The corresponding formalism for these languages is the linearly bounded nondeterministic Turing machine, which can only write on the tape cells that contained the input word in the beginning.
This and an equivalent extension of second-order logic will be proven in~\cref{subsec:des-context-sensitive-languages}.
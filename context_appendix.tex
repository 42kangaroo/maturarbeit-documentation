%! suppress = UnresolvedReference
%! suppress = MissingImport
\chapter{Mathematical Context and further proofs}\label{ch:mathematical-context-and-further-proofs}

\section{Formal languages}\label{sec:formal-languages-app}

\subsection{Regular Languages}\label{subsec:regular-languages}

The regular languages have the most restricted type of grammars.
Formally, any regular language can be described by a grammar with rules in $V\times(\Sigma \cup \Sigma V \cup \varepsilon)$.
This means that we only have exactly one non-terminal on the left-hand side and the right-hand side is either a terminal, the empty word or a terminal symbol followed by a non-terminal symbol.

\begin{exmp}
    Consider the grammar $\langle \{S, O\}, \{a\}, R, S \rangle$ with
    \[
        R = \begin{Bmatrix*}[l]
                S \to aO,
                &S \to \varepsilon, \\
                O \to aS
        \end{Bmatrix*}
    \]
    The generated language are exactly all words with even length.
\end{exmp}

The regular languages have been studied quite thoroughly and have multiple equivalent formalisms:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item The language is recognized by a Deterministic finite automaton, which process the input word one character at a time
    \item The language can be decided by a read-only Turing machine, that is a Turing machine that can not modify it's tape
    \item The language can be described by a regular expression
\end{itemize}

For a more in-depth analysis of regular languages and equivalent formalisms refer to~\cref{subsec:des-regular-languages} and~\cite{Straubing1994}.

\subsection{Context-Free Languages}\label{subsec:context-free-languages}

The context-free languages extend the regular languages by allowing arbitrary right-hand sides for the rules of the defining grammar.
Formally, that gives us rules in $V\times(\Sigma \cup V)^{*}$.
Most valid arithmetic expressions, logical formulas and formally correct code in programming languages are context-free, as we can see the non-terminal symbols as types which are then converted to specific expressions of that type.

\begin{exmp}
    Consider the grammar $\langle \{\bold{Exp}, \bold{NumF}, \bold{Num}\}, \{0, 1, (, ), -, +\}, R, \bold{Exp} \rangle$ with
    \[
        R = \begin{Bmatrix*}[l]
                \bold{Exp} \to \bold{NumF},
                &\bold{Exp} \to (\bold{Exp} + \bold{Exp}), \\
                \bold{Exp} \to (\bold{Exp} - \bold{Exp}),
                &\bold{Exp} \to (-\bold{Exp}), \\
                \bold{Num} \to 0\bold{Num},
                &\bold{Num} \to 1\bold{Num}, \\
                \bold{Num} \to \varepsilon,
                &\bold{NumF} \to 0, \\
                \bold{NumF} \to 1\bold{Num}
        \end{Bmatrix*}
    \]
    This generates the language of all well-formed formulas using addition and subtraction over binary numbers.
    For clarity, $\bold{Exp}$ denotes an arbitrary expression, $\bold{NumF}$ any number without leading zeroes and $\bold{Num}$ any number (possibly empty or with leading zeroes).
\end{exmp}

Those languages have less known formalisms, the Push-Down Automaton (again see~\cite{theory-cs}) being the most common.
For a characterization of the context-free languages using logic, see~\cref{subsec:des-context-free-languages}.

\subsection{Recursive Languages}\label{subsec:recursive-languages}

The recursive languages are the most general languages in the hierarchy, as they don't have any restrictions on the rules.
It can be shown that this set of languages is equivalent to the languages recognizable by a Turing machine.
By the Church-Turing thesis, this means that these are exactly the languages that can be computed by any of our computers and algorithms.
Thus, we have a huge number of equivalent formalisms, including a RAM machine, while-programs and lambda calculus.

It is worth noting that there are languages which are not recursive.
One of the most important example of these languages is the set of all (descriptions) of Turing machines which halt on every input, also known as the halting problem.
For the characterization using logic, again refer to~\cref{subsec:des-recursive-languages}.

\section{Descriptive Complexity}\label{sec:descriptive-complexity-context}

\subsection{Ehrenfeucht-Fraïssé Games}\label{subsec:ehrenfeucht-fraisse-games}

Ehrenfeucht-Fraïssé games are combinatorial games which have a strong connection to first-order formulas and their extensions.
Using these games, it is often possible to show inexpressibility results for certain problems in some logic $\mathcal{L}$.

As a motivation, we can look at what it means for a formula to hold on some structure.
Assume the formula has the form $\forall x\varphi$.
This can be seen as some opponent choosing some element $a \in |\mathcal{A}|$ and us then needing to show that $\varphi(x / a)$ holds.
The case where the formula has the form $\exists x\psi$ can be treated similarly, but we can select the element ourselves.

\begin{define}[Ehrenfeucht-Fraïssé Game]
    The \emph{$k$-pebble Ehrenfeucht-Fraïssé Game $\mathcal{G}_k$} is played by two players, the Spoiler and the Duplicator, on a pair of structures $\mathcal{A}$ and $\mathcal{B}$ using $k$ pairs of pebbles.
    In each move, the Spoiler places one of the remaining pebbles on an element of one of the two structures.
    Then, the Duplicator tries to match the move by placing the corresponding pebble on an element of the other structure.
    For a pebbled element $x$ in $\mathcal{A}$, we denote by $p(x)$ the corresponding pebbled element in $\mathcal{B}$.
    We say that the Duplicator wins the $k$-pebble Ehrenfeucht-Fraïssé Game on $\mathcal{A}, \mathcal{B}$ if after the $k$ rounds, the map $i : |\mathcal{A}| \to |\mathcal{B}|$ defined as
    \[
        i(x) = \begin{cases}
                   c_{j}^{\mathcal{B}} & x = c_{j}^{\mathcal{A}} \\
                   p(x) & x\text{ is pebbled}\\
                   undefined & \text{otherwise}
        \end{cases}
    \]
    is a partial isomorphism.
    A partial isomorphism is an isomorphism over some subset of the universe with all relations restricted to that subset.
\end{define}

In the Ehrenfeucht-Fraïssé Game, the Spoiler wants to show that $\mathcal{A}$ and $\mathcal{B}$ are different, whereas the Duplicator wants to show that they are equivalent.

As this is a zero-sum game with full information, one of the two players must have a winning strategy.
It can be proven that the Duplicator has a winning strategy for the $k$-pebble Ehrenfeucht-Fraïssé on $\mathcal{A}$ and $\mathcal{B}$ if and only if $\mathcal{A}$ and $\mathcal{B}$ agree on all formulas with at most $k$ nested quantifiers.
We can use these facts to prove the inexpressibility of some problems in first-order logic.
Assume we have a decision problem $\mathcal{C}$.
If we can exhibit two structures $\mathcal{A}_k \in \mathcal{C}$ and $\mathcal{B}_k \not \in \mathcal{C}$ for each $k$ and a winning strategy for the Duplicator on these two structures, we show that no first-order formula defining $\mathcal{C}$ can exist.
This methodology can be extended to other logics by adding new moves or restrictions to the game.


\subsection{Reduction and Completeness}\label{subsec:reduction}

A reduction can informally be seen as a method of using a problem we already solved to solve a new problem by converting this new problem into an instance of the old problem.
These reductions can be very useful to define complete problems for complexity classes, which in turn enables us to prove theorems for all problems of a specific complexity class.

\begin{define}[first-order reduction]
    Let $\mathcal{C}$ be a complexity class and $A$ and $B$ be two problems over vocabularies $\sigma$ and $\tau$.
    Now suppose that there is some first-order query $I: \text{STRUC}[\sigma] \to \text{STRUC}[\tau]$ for which we have the following property:
    \[
        \mathcal{A} \in A \Leftrightarrow I(\mathcal{A}) \in B
    \]
    Then $I$ is a first-order reduction from $A$ to $B$, denoted as $A \leq_{fo} B$.
\end{define}

First-order reductions can then be used to show that some problem is also a member in some complexity class, as in most complexity classes, we can compute the first-order query, and then we are left with a problem that we already know is in the required class.
The converse can also be shown: for some problem $B$ which is not in some complexity class $\mathcal{C}$, if we have $B \leq_{fo} A$, then $A$ is also not in $\mathcal{C}$, as otherwise $B$ would also be in $\mathcal{C}$, which is a contradiction.

Using the reductions, we can define completeness.

\begin{define}[Completeness via first-order reductions for Complexity Class $\mathcal{C}$]
    We say some problem $A$ is complete for $\mathcal{C}$ via $\leq_{fo}$ if and only if
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item $A \in \mathcal{C}$
        \item for all $B \in \mathcal{C}$, we have $B \leq_{fo} A$
    \end{itemize}
\end{define}

Informally, a complete problem captures the essence of the complexity class.
Further, they have an application in some proofs of equivalences between complexity classes $\mathcal{C}$ and logics $\mathcal{L}$.
These proofs follow the following steps as in~\cite{descriptive-complexity}:
\begin{enumerate}
    \item Show that $\mathcal{L} \subseteq \mathcal{C}$ by providing a way to convert any formula $\varphi \in \mathcal{L}$ into an algorithm in $\mathcal{C}$.
    \item Find a complete problem $T$ for $\mathcal{C}$ via first-order reductions.
    \item Show that $\mathcal{L}$ is closed under first-order reductions, that is that any formula can be extended by first-order quantifiers and boolean connectives and stay in $\mathcal{L}$.
    \item Find a formula for $T$ in $\mathcal{L}$, which shows $T \in \mathcal{L}$.
\end{enumerate}
The above steps work, as for any problem $B$ in $\mathcal{C}$, there is a first-order reduction $I$ to $T$, and both $\mathcal{L}$ and $\mathcal{C}$ are complete via these reductions, so we also have $B \in \mathcal{L} = \mathcal{C}$.

\subsection{Space Hierarchy Theorem}\label{subsec:space-hierarchy-theorem}

The space hierarchy theorem states that for both nondeterministic and deterministic space, we have problems that can be solved using $t(n)$ space, but not with any tighter space bound.
Formally, we have
\[
    \text{DSPACE}[o(t)] \subsetneq \text{DSPACE}[\mathcal{O}(t)]
\]
where $o(t)$ is the set of functions $f$ such that $f \in \mathcal{O}(t)$ but $t \not \in \mathcal{O}(f)$, that is all functions that grow more slowly than $t$.
This holds for all space constructible $t \geq \log n$.
The same holds for NSPACE\@.

We will present a proof for deterministic space.
\begin{proof}
    The proof uses a diagonalization argument by presenting some machine $D$ that takes a Turing machine $M$ and an input size in unary as input and does the opposite of $M$ if it halts.
    We want to show that for all $M$ which run in space $f(n) \in o(t(n))$, we have an input on which $D$ and $M$ do not agree.
    This would show that the language computed by $D$ is not in DSPACE[$o(t)$], and thus the strict containment.

    On input $\langle M, 1^{k} \rangle$ our machine $D$ marks of $t(|\langle M, 1^{k} \rangle|)$ tape cells, which are the cells that are allowed for the computation.
    Further we also maintain a counter with size $|M|\cdot 2^{t(|\langle M, 1^{k} \rangle|)}$, which is the maximum amount of different configurations a Turing machine can pass before looping on a binary tape of size $t(|\langle M, 1^{k} \rangle|)$.
    Then, we simulate $M$ on input $\langle M, 1^{k} \rangle$.
    If we transcend any bound, we reject.
    For all $M$ in DSPACE[$o(t)$], there is a $k$ such that $f(n) \leq t(n)$ by definition.
    On this input, the simulation finishes, and we can invert the output.

    This directly gives us an input for which $M$ and $D$ differ, and thus proves our claim.
    Furthermore, $D$ runs in DSPACE[$\mathcal{O}(t)$] as by construction we assured that we do not run infinitely and that we stay within the space bound.
\end{proof}



\subsection{Regular Languages}\label{subsec:des-regular-languages}

Here, we will show that the regular languages are captured exactly by second-order logic where we restrict ourselves to quantify only over predicates of arity one and do not include $\leq$.
Further, we also are not allowed to use $\leq$, but have access to equality $x = y$ and the successor relation $x = y + 1$.
We call this class SOM[$+1$].

First we need to present a formal definition of deterministic finite automata.
\begin{define}[DFA]
    A deterministic finite automaton is a 5-tuple $M = \langle Q, \Sigma, \delta, q_0,  F \rangle$ where
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item[$Q$] is the set of states
        \item[$\Sigma$] is the alphabet
        \item[$\delta$] is the transition function mapping a state and a symbol to the next state, so formally $\delta : Q\times \Sigma \to Q$.
        \item[$q_0$] the start state
        \item[$F$] a subset of $Q$ which are the accepting states.
    \end{itemize}
\end{define}
We say that a DFA $D$ accepts a word $w \in \Sigma^{*}$ if when starting at the start state, if we go through $w$ and always transition to the next state according to the actual symbol in $w$ and the actual state, we end up in an accepting state.

In~\cite{theory-cs} and~\cite{Straubing1994} there is a proof of the following fact we will use in our proof for SOM[$+1$]:
\begin{theorem}
    For any alphabet $\Sigma$, there is a DFA recognizing language $L \subseteq \Sigma^{*}$ if and only if it is regular.
\end{theorem}

Now we can start to prove our main theorem for regular languages.
\begin{theorem}
    For any alphabet $\Sigma$, a language $L \subseteq \Sigma^{*}$ is expressible in SOM[$+1$] if and only if it is regular.
\end{theorem}

\begin{proof}
    First we show that any regular language can be expressed in SOM[$+1$].
    Let $L$ be regular, end $D_L$ be a DFA recognizing the language.
    We assume $L$ does not contain the empty word, otherwise we can recognize the language $L \setminus \{\varepsilon\}$ and then add $\varphi \lor \forall x(x \neq x)$, which adds the empty string back.

    Now let $D_L$ have $k$ states.
    We can existentially quantify unary relations $X_1, \dots, X_k$ to have the meaning that $X_i(y)$ is true if and only if $D_L$ is in state $i$ after $y$ steps.
    Then, we need to make consistency checks.
    We present formulas for each of the consistency checks, and then can take the ``and'' of those to get our final formula $\exists X_1, \dots, X_k(\varphi_1 \land \varphi_2 \land \varphi_3)$.
    \begin{description}
        \item[The start state is $q_j$] We have \[\varphi_1 \coloneqq \bigwedge_{i = 1}^{k} (i = j \leftrightarrow X_j(0))\]
        \item[We end in an accepting state] Let $T_i$ be the set of all characters which lead from $q_i$ to an accepting state.
        Then we have
        \[
            \varphi_2 \coloneqq \bigwedge_{i = 1}^{k}\left(X_i(\max) \to \bigvee_{a \in T_i} Q_a(\max)\right)
        \]
        \item[We move according to the transition function] We have
        \begin{align*}
            \forall x\left( \forall y \left( y = x + 1 \to \left(\bigwedge_{i = 1}^{k} \bigwedge_{a \in \Sigma} \left(\left(X_i(x) \land Q_a(x)\right) \to X_{\delta(i, a)}(y)\right) \right. \right. \right. \\
            \left. \left. \left.\land \bigwedge_{i = 1}^{k}\bigwedge_{a \in \Sigma} \left(\left(X_i(y) \land Q_a(x)\right) \to \bigvee_{r = 1}^{k}\left(X_r(x) \land \delta(r, a) = j\right)\right)  \right)  \right) \right)
        \end{align*}
    \end{description}
    By induction, we can show that always exactly one $i$ satisfies $X_i(x)$ for any $x$.
    Thus, if the created formula is satisfied, we know that $D_L$ accepts the word, and thus we have described $L$ in SOM[$+1$].

    \vspace{5mm}

    For the other direction, we need to introduce two new concepts.

    One of them is the nondeterministic finite automaton, which is analogous to the nondeterministic Turing machine as it can also have multiple transitions going from the same state.
    As with the nondeterministic Turing machine and the Turing machine, both the DFA and the NFA have the same expressive power.

    The other concept is that of $(\mathcal{V}_1, \mathcal{V}_2)$-structures.
    These structures are generalizations of our former vocabulary $\sigma$ as they have characters in $A \times \mathcal{P}(\mathcal{V}_1)\times \mathcal{P}(\mathcal{V}_2)$.
    These structures are useful as we can make $\mathcal{V}_1$ to be the set of free first-order variables in a formula $\varphi$ and $\mathcal{V}_2$ be the set of free second-order variables in the formula.
    If at a position $i$ in our $(\mathcal{V}_1, \mathcal{V}_2)$-structures we have $x$ in the first-order component of its character, we see this as meaning that $x = i$.
    For the second-order variables in the third component, an $X$ at position $i$ means that $X(i)$ holds.

    Now, we can prove by induction that all formulas in SOM[$+1$] with free variables in $\mathcal{V}_1$ and $\mathcal{V}_2$ are regular.
    Sentences, the formulas without free variables are the special case where $\mathcal{V}_1, \mathcal{V}_2 = \emptyset$.

    First, we need to check that the $(\mathcal{V}_1, \mathcal{V}_2)$-structures are consistent, and no first-order variable $x$ appears more than once.
    This can be done by a NFA which has one state for each subset of variables, and extends its subset while going over the string.
    If a variable appears twice, we enter a state that always loops and rejects.

    Then, we see that the atomic formulas can be checked, as $x = y$, $x = y + 1$ and $Q_a(x)$ are easy to check, and checking $X(x)$ is equivalent to looking if the occurrence of $x$ has $X$ in the third component.
    We always need to take the intersection with the NFA which checks if the structure is valid.

    All boolean connectives are also valid, as regular languages are closed under complement, intersection and union as seen in~\cite{theory-cs}.

    The most difficult case is a formula of the form $\exists x \varphi$ (as $\forall x \varphi \equiv \neg \exists x \neg \varphi)$).
    If $\exists x \varphi$ is over $(\mathcal{V}_1, \mathcal{V}_2)$-structures, then $\varphi$ is over $(\mathcal{V}_1 \cup \{x\}, \mathcal{V}_2)$-structures.
    By induction, we know that $\varphi$ defines a regular language and thus there is a NFA $N$ which recognizes it.
    For the new automaton, we duplicate our states, with the meanings ``used $x$'' and ``not used $x$''.
    If we are in a state where $x$ was used, we can not take any transition with $x$ in the second set.
    If we are in a state where $x$ was not used, we can take a transition with $x$ in the second set and go to the corresponding state with $x$ used or take a transition where $x$ is not used and go to the corresponding state where $x$ was not used.

    The remaining case with second-order variables is treated analogously, without the restriction on the number of times the variable is used, so we do not need to duplicate our states.

    By induction, we have thus showed the other direction, and we see that SOM[$+ 1$] and the regular languages are equivalent.
\end{proof}

\subsection{Context-Free Languages}\label{subsec:des-context-free-languages}

For the context-free languages, we will show that they have an underlying structure that includes matchings.

A matching relation is a binary relation $M$ on the universe $\{0, \dots, n - 1\}$ which has the following properties:
\begin{description}
    \item[increasing] If $M(i, j)$, then $i < j$
    \item[uniqueness] Any $k$ in the universe appears at most once in the relation
    \item[non-crossing] If we were to draw arcs for the matching, none of them would intersect.
    Formally, if $M(i, j)$ and $M(k, l)$, then either $j < k$ or $l < i$
\end{description}
Visually, we can think of these relations as nested ranges over the universe.

With matchings, we can define FO($\exists$Match) as the first-order logic extended with existential quantification over matching relations.

\begin{theorem}
    For any alphabet $\Sigma$, a language $L \subseteq \Sigma^{*}$ is expressible in FO($\exists$Match) if and only if it is context-free.
\end{theorem}

For this, we first introduce a normal form for context-free grammars.
The proof that every context-free grammar can be converted to this form can be found in~\cite{Lautemann1995}.
\begin{lemma}
    Every context free language has a grammar which satisfies
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item All rules are of one of the two forms
        \begin{itemize}
            \setlength\itemsep{0.2em}
            \item $S \to \alpha$ with $\alpha \in \Sigma$
            \item $X \to \alpha u \beta$ with $\alpha, \beta \in \Sigma$ and $u \in (\Sigma \cup V)^{*}$
        \end{itemize}
        \item For all production rules with a right-hand side that has at least one non-terminal we define its \emph{pattern}.
        The pattern of rule $X \to v_{0}X_{1}v_{2}X_{2}\dots X_{s}v_{s}$ is defined to be $v_{0}|v_{1}|\dots|v_{s}$, where $|$ is a new symbol not in $\Sigma$
        We require that for any two rules with the same pattern, they have the same left-hand side and thus the source non-terminal can be uniquely identified by the pattern.
    \end{itemize}
\end{lemma}

For a specific arch $\langle i, j \rangle$ in a matching $M$ on some word structure, we can also determine a pattern.
For this, we go through all indices from $i$ to $j$ and add the character at the actual position.
If we are at a starting point of some arch $\langle k, l \rangle$, instead of adding the actual character, we add $|$ and continue at $l + 1$.

Now, we can start with the proof of our theorem.
\begin{proof}
    We want to find a formula such that for all arches $\langle i, j \rangle$ in $M$, the substring from $i$ to $j$ can be derived from the non-terminal for which we have a rule with the pattern of $\langle i, j \rangle$.

    For this, we say that arch $\langle i, j \rangle$ \emph{corresponds} to a production rule $p$ if they have the same pattern.
    For any string $u$, we have a formula $\phi_u(i, j)$ which means that the substring from $i$ to $j$ is $u$.
    This formula is easy to write as we only need to check each character one by one using the successor relation.
    We can express correspondence to $p \equiv X \to v_{0}X_{1}v_{2}X_{2}\dots X_{s}v_{s}$ (including rules with terminal right-hand side) by the following formula:
    \begin{align*}
        \mathcal{X}_p(x, y) \equiv~&\exists x_1, y_1, \dots, x_s, y_s \left(\left(x < x_1 \land x_1 < y_1 \land y_1 < x_2 \land \dots \land y_s < y \right) \land \right. \\
        &\left. \left(\phi_{v_0}(x, x_1 - 1) \land \phi_{v_1}(y_1 + 1, x_2 - 1) \land \dots \land \phi_{v_s}(y_s + 1, y)\right) \land \right. \\
        &\left. \left(M(x_1, y_1) \land \dots \land M(x_s, y_s) \land M(x, y) \right) \land \right. \\
        &\left. \forall k, l \left(M(k, l) \to \left((x \leq k \land l \leq y) \lor (x_1 \leq k \land l \leq y_1) \lor \dots \lor (x_s \leq k \land l \leq y_s)\right)\right) \right)
    \end{align*}
    The first line means that the $v_i$ are in the right order, the second that they do correspond, the third that there are arches between the $v_i$ and the last that there are no other arches.

    Now, we want to be more general, and express the that the pattern of $\langle i, j \rangle$ corresponds to some production rule with left-hand side $X$.
    Let $\widetilde{\mathcal{X}}_X(x, y)$ be the disjunction of all $\mathcal{X}_p$ with left-hand side $X$.
    Because our normal form says that the pattern uniquely determines the left-hand side, for each arch which has arches underneath, we have a unique corresponding non-terminal.

    Now, we want to have a formula which expresses not only correspondence for a production rule $p$, but also that the non-terminals are correct.
    For this, we supplement our formula $\mathcal{X}_p$ with a new line, expressing that the non-terminals correspond.
    \begin{align*}
        \widetilde{\mathcal{X}}_p(x, y) \equiv~&\exists x_1, y_1, \dots, x_s, y_s \left(\left(x < x_1 \land x_1 < y_1 \land y_1 < x_2 \land \dots \land y_s < y \right) \land \right. \\
        &\left. \left(\phi_{v_0}(x, x_1 - 1) \land \phi_{v_1}(y_1 + 1, x_2 - 1) \land \dots \land \phi_{v_s}(y_s + 1, y)\right) \land \right. \\
        &\left. \left(M(x_1, y_1) \land \dots \land M(x_s, y_s) \land M(x, y) \right) \land \right. \\
        &\left. \forall k, l \left(M(k, l) \to \left((x \leq k \land l \leq y) \lor (x_1 \leq k \land l \leq y_1) \lor \dots \lor (x_s \leq k \land l \leq y_s)\right)\right) \right. \\
        &\left. \left(\widetilde{\mathcal{X}}_{X_1}(x_1, y_1) \land \dots \land \widetilde{\mathcal{X}}_{X_s}(x_s, y_s)\right)\right)
    \end{align*}

    Finally, with $P$ being our set of rules, we have a formula that tells us a word can be derived from the start symbol:
    \[
        \exists M \left(\widetilde{\mathcal{X}}_S(0, \max) \land \forall x, y \left(M(x, y) \to \bigvee_{p \in P} \widetilde{\mathcal{X}}_p(x, y) \right) \right)
    \]

    Now, by construction, if and only if a word satisfies the formula, there is a derivation from $S$ for the word, as each arch can be seen as a derivation step, which we check is valid with our formula.
    We used our assumption from the normal form to show that the $\widetilde{\mathcal{X}}_p$ are only satisfied when the arches which are non-terminal do not belong to any other non-terminal symbol then the one in the rule.
    If two terminal rules coincide, we do not care as no further derivation is possible.

    \vspace{5mm}

    Now, we come to the other direction.
    This direction requires some new notation and lemmas.
    We will use the notion of tree languages.
    \begin{define}[Tree Language]
        In a tree language, we have a rooted tree, which has an order on its vertices in leftmost depth-first way\footnote{The order which we find by doing a depth-first search on the tree, entering the leftmost node first}.
        Each node has a label, and each label has an arity which corresponds to the out-degree of the node.
        A tree language is the set of all trees over a finite label set.

        We define the \emph{leaf alphabet} of a tree language to be the set of $0$-ary labels.
        For any tree $T$ in a tree language, we define the \emph{yield} of $T$ to be the leaf labels concatenated according to the order relation from left to right.

        The vocabulary of a tree language $\mathcal{T}$ is $\tau = \langle \{0, \dots, n - 1\}, Q_a, Q_b, \dots, Q_z, \leq , 0, 1, \max , C^2 \rangle$
        In addition to the relations for each label, there is a child relation $C(i, j)$ which means ``node $i$ is a child of node $j$''.
    \end{define}

    Now, we present two lemmas for recognizable tree languages and their relation to context-free languages.
    \begin{lemma}[\cite{Mezei1967}]
        A language $L \subseteq \Sigma^{*}$ is context free if and only if there is a recognizable tree language $T$ with leaf alphabet $\Sigma$ for which a word is in $L$ if and only if it is the yield of some tree in $T$.
    \end{lemma}

    \begin{lemma}[\cite{Thatcher1968}]
        A tree language $T$ is recognizable if and only if there is a monadic second-order sentence that recognizes it.
    \end{lemma}

    We now present some relations that can be written in MSO on trees.
    \begin{description}
        \item[Lf($i$)] Node $i$ is a leaf $\text{Lf}(i) \equiv \forall x \neq C(x, i)$
        \item[Lc($i, j$)] Node $i$ is the leftmost child of $j$ $\text{Lc}(i, j) \equiv C(i, j) \land \forall x (C(x, j) \to i < x)$
        \item[Rc($i, j$)] Node $i$ is the rightmost child of $j$ $\text{Rc}(i, j) \equiv C(i, j) \land \forall x (C(x, j) \to x < i)$
        \item[An($i, j$)] Node $i$ is an ancestor or $j$
        \begin{align*}
            \text{An}(i, j) \equiv~&\exists U (U(i) \land U(j) \land \forall x (U(x) \to \\
            &((x \neq i \leftrightarrow \exists y (C(x, y) \land U(y))) \land (x \neq j \leftrightarrow \exists y (C(y, x) \land U(y))))))
        \end{align*}
        \item[Pt($U, i, j$)] Node $i$ is an ancestor or $j$, $j$ is a leaf and $U$ contains all nodes in the path from $i$ to $j$.
        \begin{align*}
            \text{Pt}(U, i, j) \equiv~&U(i) \land U(j) \land \text{Lf}(j) \land \forall x (U(x) \to \\
            &((x \neq i \leftrightarrow \exists y (C(x, y) \land U(y))) \land (x \neq j \leftrightarrow \exists y (C(y, x) \land U(y)))))
        \end{align*}
    \end{description}

    Using these two lemmas, we can continue by presenting for any formula $\varphi$ in FO($\exists$Match) a formula $\phi$ in MSO over trees such that
    \[w \models \varphi \Leftrightarrow \text{there exists a tree $T$ with yield $w$ such that } T \models \phi\]

    For this, we present a class of trees which correspond to a word with a matching.
    For any word $w$ with matching $M$, we can construct a tree over $\Sigma \cup \{\oplus^2, \odot^2\}$.
    We do this using an intermediate step.
    First, we construct a tree with wrong arity for nodes of type $\oplus$ by assigning one node of type $\oplus$ to each arch, with edges to every direct arch underneath it and every character directly underneath it.
    If we have multiple trees, we add a new $\odot$ node on top with an edge to all roots of these trees.
    To fix the arity issue, we repeat the following procedure until there are no nodes with more than outdegree 2.

    Take some node with outdegree greater than 2.
    Take the two leftmost children and add a $\odot$ node with an edge to both, and an edge from the new node to the former parent.
    This procedure will eventually terminate as we always decrease the outdegree of some node by one and add a valid node.

    We can see that this procedure can also be done backward if and only if for any node of type $\oplus$, the leaf on the leftmost and rightmost path of a node are distinct and no $\oplus$ node occurs on the path between the two.
    We can express this property of a tree by the following formula.
    \begin{align*}
        \Upsilon \equiv~& \forall x ( Q_{\oplus}(x) \to ( \exists y, z, U_y, U_z (y \neq z \land \text{Pt}(U_z, x, z) \land \text{Pt}(U_y, x, y) \land \\
        &\forall r (U_y(r) \to ((r = y \lor \exists w (U_y(w) \land \text{Rc}(w, r)))\land (r = y \lor r = x \lor Q_{\odot}(r)))) \land \\
        &\forall r (U_z(r) \to ((r = z \lor \exists w (U_z(w) \land \text{Lc}(w, r)))\land (r = y \lor r = x \lor Q_{\odot}(r)))))))
    \end{align*}
    Here, in the first line we assert that for every node with label $\oplus$, we have two distinct leaves $y, z$ with paths $U_y$ and $U_z$.
    The second and third line assert the same for $x$ and $y$, that they are the rightmost~/~leftmost leaf and that no other $\oplus$ type node lies on the path from them to $x$.

    Now, we want to convert our formula $\varphi$ over strings with a matching to a formula $\gamma$ over trees.
    Then we can assert that the tree represents a string with matching and the yield satisfies $\varphi$ using $\Upsilon \land \gamma$.

    For this, we need to restrict any quantifiers in $\varphi$ to the leaves by replacing $\exists x \phi$ with $\exists x \text{Lf}(x) \land \phi$ and $\forall x \phi$ with $\forall x \text{Lf}(x) \to \phi$.
    Further, we replace $M(z, y)$ by
    \begin{align*}
        m(z, y) \equiv~& \exists x ( Q_{\oplus}(x) \land ( \exists U_y, U_z (y \neq z \land \text{Pt}(U_z, x, z) \land \text{Pt}(U_y, x, y) \land \\
        &\forall r (U_y(r) \to ((r = y \lor \exists w (U_y(w) \land \text{Rc}(w, r)))\land (r = y \lor r = x \lor Q_{\odot}(r)))) \land \\
        &\forall r (U_z(r) \to ((r = z \lor \exists w (U_z(w) \land \text{Lc}(w, r)))\land (r = y \lor r = x \lor Q_{\odot}(r)))))))
    \end{align*}
    which is very similar to $\Upsilon$.

    This is already everything we need to do, and thus we can conclude that
    \[w \models \varphi \Leftrightarrow \text{there exists a tree $T$ with yield $w$ such that } T \models \Upsilon \land \gamma \]
    Thus, we have proved both directions and see that the context free languages are exactly captured by FO($\exists$Match)
\end{proof}

\subsection{Context-Sensitive Languages}\label{subsec:des-context-sensitive-languages}

Here, we show the equivalence between context-sensitive languages and nondeterministic Turing machines using $\mathcal{O}(n)$ space.
The two directions of the proof were presented separately in~\cite{Kuroda1964} and~\cite{Landweber1963}.

\begin{theorem}
    \label{thm:nspacecontextsensitive}
    The class of context-sensitive languages is exactly the class of languages accepted by a linear bounded nondeterministic Turing machine.
\end{theorem}

\begin{proof}
    For the direction from grammar to Turing machine, we only need to show that our Turing machine can simulate a derivation backwards.
    We know that every context-sensitive language has a noncontracting grammar $G$.
    Using this fact, we can construct a nondeterministic Turing machine $N$ which scans the current tape and whenever it recognizes a pattern of the right-hand side of a production rule in $G$, it decides whether it replaces it or not by the left-hand side of the rule.
    If some computation branch of $N$ ends up with only the start symbol of $G$ on the tape, we accept.
    Essentially, $N$ simulates a derivation of $w$ from the start symbol backwards.
    Because we try all possibilities by nondeterminism, we know that if $N$ does not accept $w$, there is no derivation ending in $w$ from the start symbol of $G$.
    As we assumed $G$ is noncontracting, replacing the right-hand side of a rule with the left-hand side never makes the word longer, and thus we only need $\mathcal{O}(|w|)$ space.

    \vspace{5mm}

    The other direction works by explicitly defining a grammar which simulates any linear bounded automaton backwards.
    Without loss of generality, we include an end marker $\#$.

    Let $N = \langle Q, \Sigma, \Gamma, \delta, q_0, q_{accept}, q_{reject} \rangle$ be a NSPACE[$\mathcal{O}(n)$] Turing machine.
    Then, we construct a grammar $G = \langle V, \Sigma, P, S \rangle$ with $V = \bigcup_{q_i \in Q} \bigcup_{a_j \in \Gamma} \{b_{q_i, a_j}\} \cup \{S, L, R, \#\} \cup \bigcup_{a_w \in \Gamma \setminus \Sigma} \{a_w\}$.
    The $b_{q, a}$ represent a position on the tape including the actual state and the actual character, in addition we also have the start state, the end marker and some utility non-terminals.

    We now add the following rules to $P$:
    \begin{itemize}
        \setlength\itemsep{0.2em}
        \item For each $a_i \in \Gamma$, we add a rule $S \to Lb_{q_{accept}, a_i}R$ to $P$.
        These rules mean that we are in a final accept state.
        To extend the final tape, we add again for each $a_i \in \Gamma$ the rules $L \to La_i$, $L \to \#$, $R \to a_{i}R$ and $R \to \#$ to our rule set.
        These rules allow derivations from $S$ to an end tape of the form $\#a_{i_1}\dots b_{q_{accept}, a_{i_j}}\dots a_{i_k}\#$.
        \item For each rule $\langle a_k, q_l, L \rangle \in \delta(q_i, a_j)$, we add rule $b_{a_w, q_l}a_k \to a_{w}b_{a_j, q_i}$ for every $a_w \in \Gamma$.
        Similarly, for each rule $\langle a_k, q_l, R \rangle \in \delta(q_i, a_j)$, we add rule $a_{k}b_{a_w, q_l} \to b_{a_j, q_i}a_{w}$ for every $a_w \in \Gamma$.
        We can clearly see that these rules simulate the nondeterministic Turing machine backwards.
        \item For the start, we include $\#b_{a_i, q_0} \to \#a_i$ for all $a_i \in \Sigma$
        This rule allows us to say that we are at the start of our computation and ``remove'' the read-write head to get our initial input word.
    \end{itemize}
    As for any word in $G$ we can follow back the derivation to an accepting state, we have that both $N$ and $G$ define the same language.

    Thus, we are done and have proven the equivalence of context-sensitive languages and linear bounded automata.
\end{proof}


\subsection{Recursive Languages}\label{subsec:des-recursive-languages}

The most general case is interesting as it gives us a logical formalism for all problems which are computable at all.

The proof relies on Diophantine sets.
Those sets are the sets that correspond to the tuples which have a solution for some Diophantine equation.
A Diophantine equation is a polynomial equation $P$ with a tuple $\overline{x}$ of parameters and a tuple $\overline{y}$ of variables.
A tuple $\overline{x}$ has a solution if there exists a tuple $\overline{y}$ such that $P(\overline{x}, \overline{y}) = 0$.
The famous MRDP Theorem states that the Diophantine sets are exactly the computable sets.
At the same time, this shows that Hilbert's 10$^{th}$ problem is unsolvable.
A full proof of these facts can be found in~\cite{Matijasevic1996}.

With this new characterization, there is a quite straightforward characterization of computable sets.
The logic FO$(\exists \mathbb{N})$ consists of all formulas $\phi(\overline{x}) \equiv \exists \overline{y} (\varphi(\overline{x}, \overline{y}))$ with $\varphi$ having only bounded quantifiers (of the form $\exists x < y$ or $\forall x < y$), addition, multiplication, equality, and any constant natural number~\cite{Entropy2020}.
The existential quantifiers at the beginning are allowed to range over all the natural numbers.

These formulas can define exactly the Diophantine sets, as the formulas we present are exactly those that mean ``there is a tuple $\overline{y}$ of natural numbers such that some polynomial is satisfied''.
Thus, they also define exactly all computable sets.

\subsection{Open questions}\label{subsec:open-questions}

The domain of Descriptive Complexity is full of open questions as the proofs of lower bounds seems to be very difficult in most cases.
Further, even separation between complexity classes which seem to take an exponential amount of resources compared to another one in practice can not be shown to be different.

\subsubsection{P$\overset{?}{=}$NP}\label{subsubsec:pnp}
The P vs.~NP question is the most emblematic question in Descriptive Complexity Theory.
In practice, for any NP-complete problem, only exponential worst-case algorithms are known.
This leads to the widely believed conjuncture that P $\neq$ NP\@.
The problem is one of the seven Millennium Problems and a solution of equality or inequality is worth 1 Million US dollars.

The consequences of a solution stating that P $=$ NP could have many practical advantages if it was constructive and had a low constant, as many important problems in research and logistics could be solved quickly.
It would also mean the breakdown of most of modern cryptography, which relies on problem being intractable.
On a conceptual level, it would mean that finding a proof to a problem is not harder than verifying its correctness, which would greatly impact the work of mathematicians.
If a proof of the contrary would be known, this would focus the research more on the average case complexity of NP problems, but because of the continued lack of success on the question, this shift has already widely taken place.

\subsubsection{NSPACE[$\mathcal{O}(n)$]$\overset{?}{=}$DSPACE[$\mathcal{O}(n)$]}\label{subsubsec:nspacedspace}

This problem is known under the name first Linear bounded automaton problem since its proposal by Kuroda in~\cite{Kuroda1964}, and asks if nondeterminism adds power in the context of bounded space.
This comes from the fact that a NSPACE[$\mathcal{O}(n)$] Turing machine can be seen as a Turing machine with a linear bound on its space usage.
This theorem is of interest as we know that NSPACE[$\mathcal{O}(n)$] is equivalent to the context-sensitive languages by~\cref{subsec:des-context-sensitive-languages}.

Since the proposal, there were two advances.
One is the proof that NSPACE is closed under complement.
The contrary would have implied $\text{NSPACE}[\mathcal{O}(n)] \neq \text{DSPACE}[\mathcal{O}(n)]$ as DSPACE is closed under complement.
The second advance is Savitch's Theorem in~\cref{subsec:nspacesubsetdspacesquared} which already gives a bound for simulating NSPACE using DSPACE machines.
It is not known if this theorem is optimal, that is whether the blow-up by a power of 2 is optimal or if we can do better.

An equality would imply that the context-sensitive languages can be recognized by a deterministic linear bounded automaton, which could make recognizing words in context-sensitive languages easier and faster.